{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transport Model Classification using a Bidirectional LSTM\n",
    "## (Hyperparam Tuning)\n",
    "To better understand the code, check out `./prep_files/biLSTM_transport_classification.ipynb`. That notebook explains the thought process behind this code on a sample dataset whilst this goes straight to the point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"./custom.css\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "from src.engine import biLSTM_engine\n",
    "from src.model_transportMode import BiLSTMNetwork\n",
    "from src.dataset import TransportModeDataset\n",
    "from src.hyperparam import RayTuning\n",
    "from ray import tune, train\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Load custom CSS file\n",
    "css = HTML('<link rel=\"stylesheet\" type=\"text/css\" href=\"./custom.css\">')  # Ensure the path is correct\n",
    "display(css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─LSTM: 1-1                              8,429,568\n",
       "├─Linear: 1-2                            8,200\n",
       "├─ReLU: 1-3                              --\n",
       "=================================================================\n",
       "Total params: 8,437,768\n",
       "Trainable params: 8,437,768\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BiLSTMNetwork(input_size=6, hidden_size=512, num_layers=2)\n",
    "summary(model, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMNetwork(\n",
       "  (lstm): LSTM(6, 512, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (fc): Linear(in_features=1024, out_features=8, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [0. 1. 2. 3. 4. 5. 6. 7.]\n"
     ]
    }
   ],
   "source": [
    "y_train = pd.read_csv('./data/lstm_features_labels/train/metadata.csv')\n",
    "y_train = y_train.iloc[:,1].values\n",
    "print('Classes:', np.unique(y_train))\n",
    "\n",
    "class_weights = torch.tensor(compute_class_weight('balanced', classes=np.unique(y_train), y=y_train), dtype=torch.float32)\n",
    "class_weights = class_weights.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "del y_train"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original coarse labels are shown below. During preprocessing, the Null class (`0.0`), was dropped and with the remaining, I substracted `1` from the labels so that it would start from `0`.\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = './data/lstm_features_labels/'\n",
    "\n",
    "train_datasets = TransportModeDataset(base_dir, mode='train')\n",
    "val_test_datasets = TransportModeDataset(base_dir, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "valid_datasets, test_datasets = random_split(val_test_datasets, [0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46240, 5782, 5781)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_datasets), len(valid_datasets), len(test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Hyperparameter search space\n",
    "config = {\n",
    "    \"optimizer\": tune.choice([\"adam\", \"adamw\"]),\n",
    "    \"lr\": tune.loguniform(0.005, 0.5),\n",
    "    \"scheduler\": tune.choice([\"exp\", \"ReduceLROnPlateau\"]),\n",
    "    \"gamma\": tune.uniform(0.90, 0.95),  # for exponential scheduler\n",
    "    \"patience\": tune.choice([5, 10]),  # for ReduceLROnPlateau\n",
    "    \"epochs\": tune.choice([1200]),\n",
    "    \"batch_size\": tune.choice([32, 64, 128]),\n",
    "    \"weight_decay\": tune.loguniform(0.005, 0.5),\n",
    "    \"hidden_size\": tune.choice([50, 100, 150, 200, 250, 300]),\n",
    "    \"num_layers\": tune.choice([1, 2, 3]),  # Number of GRU layers\n",
    "    \"dropout\": tune.uniform(0.3, 0.7)  # Dropout rate\n",
    "}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "save_dir = 'BiLSTM'\n",
    "model = BiLSTMNetwork\n",
    "modelType = 'BiLSTM'\n",
    "engine = biLSTM_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytuner = RayTuning(config, save_dir, criterion, model, modelType, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-07-30 00:21:58</td></tr>\n",
       "<tr><td>Running for: </td><td>03:24:18.94        </td></tr>\n",
       "<tr><td>Memory:      </td><td>11.5/13.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=40<br>Bracket: Iter 8.000: -1.3463091565215068 | Iter 4.000: -1.6424075839269228 | Iter 2.000: -1.9465405692870945 | Iter 1.000: -2.0799787531900145<br>Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout</th><th style=\"text-align: right;\">  epochs</th><th style=\"text-align: right;\">   gamma</th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_layers</th><th>optimizer  </th><th style=\"text-align: right;\">  patience</th><th>scheduler        </th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      loss</th><th style=\"text-align: right;\">  accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_cf6ae_00000</td><td>TERMINATED</td><td>127.0.0.1:15820</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\"> 0.323233</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.9366  </td><td style=\"text-align: right;\">          150</td><td style=\"text-align: right;\">0.19591   </td><td style=\"text-align: right;\">           3</td><td>adam       </td><td style=\"text-align: right;\">         5</td><td>exp              </td><td style=\"text-align: right;\">    0.0102567 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">       1420.6   </td><td style=\"text-align: right;\">nan       </td><td style=\"text-align: right;\">  19.3705 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00001</td><td>TERMINATED</td><td>127.0.0.1:27232</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.546993</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.901029</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.00965392</td><td style=\"text-align: right;\">           1</td><td>adamw      </td><td style=\"text-align: right;\">        10</td><td>exp              </td><td style=\"text-align: right;\">    0.0132938 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        689.531 </td><td style=\"text-align: right;\">  0.871139</td><td style=\"text-align: right;\">  61.8644 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00002</td><td>TERMINATED</td><td>127.0.0.1:27104</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.482428</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.926239</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.0365477 </td><td style=\"text-align: right;\">           3</td><td>adam       </td><td style=\"text-align: right;\">         5</td><td>exp              </td><td style=\"text-align: right;\">    0.00619871</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         97.4324</td><td style=\"text-align: right;\">  2.15188 </td><td style=\"text-align: right;\">  18.8689 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00003</td><td>TERMINATED</td><td>127.0.0.1:31536</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\"> 0.326021</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.929621</td><td style=\"text-align: right;\">          250</td><td style=\"text-align: right;\">0.0291001 </td><td style=\"text-align: right;\">           1</td><td>adam       </td><td style=\"text-align: right;\">         5</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.0820464 </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        165.711 </td><td style=\"text-align: right;\">  2.0482  </td><td style=\"text-align: right;\">  16.7243 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00004</td><td>TERMINATED</td><td>127.0.0.1:676  </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.543999</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.915231</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.0669185 </td><td style=\"text-align: right;\">           3</td><td>adamw      </td><td style=\"text-align: right;\">         5</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.116798  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        167.035 </td><td style=\"text-align: right;\">  2.15228 </td><td style=\"text-align: right;\">   9.68523</td></tr>\n",
       "<tr><td>train_model_cf6ae_00005</td><td>TERMINATED</td><td>127.0.0.1:4172 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.52708 </td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.909112</td><td style=\"text-align: right;\">          300</td><td style=\"text-align: right;\">0.329314  </td><td style=\"text-align: right;\">           2</td><td>adam       </td><td style=\"text-align: right;\">        10</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.0354227 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        102.833 </td><td style=\"text-align: right;\">  6.21576 </td><td style=\"text-align: right;\">  10.844  </td></tr>\n",
       "<tr><td>train_model_cf6ae_00006</td><td>TERMINATED</td><td>127.0.0.1:1952 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.508334</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.919758</td><td style=\"text-align: right;\">          300</td><td style=\"text-align: right;\">0.177515  </td><td style=\"text-align: right;\">           3</td><td>adamw      </td><td style=\"text-align: right;\">        10</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.142401  </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        211.972 </td><td style=\"text-align: right;\">  2.08555 </td><td style=\"text-align: right;\">  14.597  </td></tr>\n",
       "<tr><td>train_model_cf6ae_00007</td><td>TERMINATED</td><td>127.0.0.1:11344</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.35637 </td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.926985</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.029945  </td><td style=\"text-align: right;\">           1</td><td>adam       </td><td style=\"text-align: right;\">        10</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.42607   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        139.761 </td><td style=\"text-align: right;\">  2.08055 </td><td style=\"text-align: right;\">   9.68523</td></tr>\n",
       "<tr><td>train_model_cf6ae_00008</td><td>TERMINATED</td><td>127.0.0.1:26172</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.61607 </td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.938612</td><td style=\"text-align: right;\">           50</td><td style=\"text-align: right;\">0.00537332</td><td style=\"text-align: right;\">           3</td><td>adam       </td><td style=\"text-align: right;\">        10</td><td>exp              </td><td style=\"text-align: right;\">    0.213743  </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        240.154 </td><td style=\"text-align: right;\">  2.07939 </td><td style=\"text-align: right;\">  19.3705 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00009</td><td>TERMINATED</td><td>127.0.0.1:19368</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.591842</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.945748</td><td style=\"text-align: right;\">          300</td><td style=\"text-align: right;\">0.0260556 </td><td style=\"text-align: right;\">           1</td><td>adamw      </td><td style=\"text-align: right;\">         5</td><td>exp              </td><td style=\"text-align: right;\">    0.00775872</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        557.375 </td><td style=\"text-align: right;\">  1.14526 </td><td style=\"text-align: right;\">  54.047  </td></tr>\n",
       "<tr><td>train_model_cf6ae_00010</td><td>TERMINATED</td><td>127.0.0.1:28648</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.608387</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.90598 </td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">0.0662913 </td><td style=\"text-align: right;\">           3</td><td>adam       </td><td style=\"text-align: right;\">        10</td><td>exp              </td><td style=\"text-align: right;\">    0.166165  </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        130.195 </td><td style=\"text-align: right;\">  2.07988 </td><td style=\"text-align: right;\">   8.09409</td></tr>\n",
       "<tr><td>train_model_cf6ae_00011</td><td>TERMINATED</td><td>127.0.0.1:27816</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.503428</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.901271</td><td style=\"text-align: right;\">           50</td><td style=\"text-align: right;\">0.131915  </td><td style=\"text-align: right;\">           1</td><td>adam       </td><td style=\"text-align: right;\">         5</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.00577868</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        239.943 </td><td style=\"text-align: right;\">  1.84425 </td><td style=\"text-align: right;\">  26.963  </td></tr>\n",
       "<tr><td>train_model_cf6ae_00012</td><td>TERMINATED</td><td>127.0.0.1:22512</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\"> 0.364489</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.937778</td><td style=\"text-align: right;\">          150</td><td style=\"text-align: right;\">0.0808732 </td><td style=\"text-align: right;\">           3</td><td>adam       </td><td style=\"text-align: right;\">        10</td><td>exp              </td><td style=\"text-align: right;\">    0.00712738</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        140.855 </td><td style=\"text-align: right;\">  2.24451 </td><td style=\"text-align: right;\">  15.6866 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00013</td><td>TERMINATED</td><td>127.0.0.1:10740</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.658437</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.943573</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">0.0195091 </td><td style=\"text-align: right;\">           3</td><td>adam       </td><td style=\"text-align: right;\">        10</td><td>exp              </td><td style=\"text-align: right;\">    0.292099  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         69.2245</td><td style=\"text-align: right;\">  2.08033 </td><td style=\"text-align: right;\">   8.09409</td></tr>\n",
       "<tr><td>train_model_cf6ae_00014</td><td>TERMINATED</td><td>127.0.0.1:22116</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\"> 0.466964</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.921355</td><td style=\"text-align: right;\">          300</td><td style=\"text-align: right;\">0.0175079 </td><td style=\"text-align: right;\">           3</td><td>adam       </td><td style=\"text-align: right;\">        10</td><td>exp              </td><td style=\"text-align: right;\">    0.263288  </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        266.422 </td><td style=\"text-align: right;\">  2.0797  </td><td style=\"text-align: right;\">  17.1221 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00015</td><td>TERMINATED</td><td>127.0.0.1:18304</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\"> 0.401566</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.947145</td><td style=\"text-align: right;\">          300</td><td style=\"text-align: right;\">0.01729   </td><td style=\"text-align: right;\">           1</td><td>adam       </td><td style=\"text-align: right;\">        10</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.0321102 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        328.308 </td><td style=\"text-align: right;\">  1.93445 </td><td style=\"text-align: right;\">  21.9128 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00016</td><td>TERMINATED</td><td>127.0.0.1:7504 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\"> 0.464415</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.907404</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">0.0493704 </td><td style=\"text-align: right;\">           2</td><td>adam       </td><td style=\"text-align: right;\">        10</td><td>exp              </td><td style=\"text-align: right;\">    0.0170817 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         68.6544</td><td style=\"text-align: right;\">  2.09968 </td><td style=\"text-align: right;\">  18.8516 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00017</td><td>TERMINATED</td><td>127.0.0.1:15924</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.604648</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.934035</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.327719  </td><td style=\"text-align: right;\">           3</td><td>adamw      </td><td style=\"text-align: right;\">         5</td><td>exp              </td><td style=\"text-align: right;\">    0.468027  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         90.6823</td><td style=\"text-align: right;\">  2.08271 </td><td style=\"text-align: right;\">  14.5797 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00018</td><td>TERMINATED</td><td>127.0.0.1:9244 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.428312</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.931615</td><td style=\"text-align: right;\">           50</td><td style=\"text-align: right;\">0.201953  </td><td style=\"text-align: right;\">           1</td><td>adam       </td><td style=\"text-align: right;\">        10</td><td>exp              </td><td style=\"text-align: right;\">    0.0589548 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         62.0327</td><td style=\"text-align: right;\">  2.13503 </td><td style=\"text-align: right;\">  15.9806 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00019</td><td>TERMINATED</td><td>127.0.0.1:27420</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.369747</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.933878</td><td style=\"text-align: right;\">          150</td><td style=\"text-align: right;\">0.260428  </td><td style=\"text-align: right;\">           3</td><td>adam       </td><td style=\"text-align: right;\">         5</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.0528635 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">       1275.03  </td><td style=\"text-align: right;\">nan       </td><td style=\"text-align: right;\">  19.3705 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00020</td><td>TERMINATED</td><td>127.0.0.1:20860</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\"> 0.403177</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.906876</td><td style=\"text-align: right;\">          300</td><td style=\"text-align: right;\">0.0542591 </td><td style=\"text-align: right;\">           2</td><td>adamw      </td><td style=\"text-align: right;\">        10</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.00843174</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        799.186 </td><td style=\"text-align: right;\">  2.59544 </td><td style=\"text-align: right;\">  31.6154 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00021</td><td>TERMINATED</td><td>127.0.0.1:22944</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\"> 0.660167</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.926483</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.460582  </td><td style=\"text-align: right;\">           1</td><td>adamw      </td><td style=\"text-align: right;\">        10</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.00767672</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         75.8071</td><td style=\"text-align: right;\">  3.11146 </td><td style=\"text-align: right;\">  22.1723 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00022</td><td>TERMINATED</td><td>127.0.0.1:2820 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.674254</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.936298</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.0180598 </td><td style=\"text-align: right;\">           3</td><td>adam       </td><td style=\"text-align: right;\">         5</td><td>exp              </td><td style=\"text-align: right;\">    0.181435  </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        175.966 </td><td style=\"text-align: right;\">  2.08122 </td><td style=\"text-align: right;\">  14.2511 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00023</td><td>TERMINATED</td><td>127.0.0.1:16164</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.519494</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.918614</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">0.0816259 </td><td style=\"text-align: right;\">           2</td><td>adamw      </td><td style=\"text-align: right;\">         5</td><td>exp              </td><td style=\"text-align: right;\">    0.442892  </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        116.081 </td><td style=\"text-align: right;\">  2.5583  </td><td style=\"text-align: right;\">  11.4666 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00024</td><td>TERMINATED</td><td>127.0.0.1:25580</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.588376</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.935609</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.487399  </td><td style=\"text-align: right;\">           3</td><td>adamw      </td><td style=\"text-align: right;\">         5</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.0223753 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         87.4375</td><td style=\"text-align: right;\">  2.0913  </td><td style=\"text-align: right;\">  14.5797 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00025</td><td>TERMINATED</td><td>127.0.0.1:24388</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.494697</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.931817</td><td style=\"text-align: right;\">          300</td><td style=\"text-align: right;\">0.00769697</td><td style=\"text-align: right;\">           1</td><td>adamw      </td><td style=\"text-align: right;\">        10</td><td>exp              </td><td style=\"text-align: right;\">    0.0756331 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        644.293 </td><td style=\"text-align: right;\">  0.746208</td><td style=\"text-align: right;\">  67.2951 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00026</td><td>TERMINATED</td><td>127.0.0.1:25984</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\"> 0.528802</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.932255</td><td style=\"text-align: right;\">          250</td><td style=\"text-align: right;\">0.194354  </td><td style=\"text-align: right;\">           3</td><td>adam       </td><td style=\"text-align: right;\">         5</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.0122873 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        123.208 </td><td style=\"text-align: right;\">  3.20823 </td><td style=\"text-align: right;\">  18.0387 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00027</td><td>TERMINATED</td><td>127.0.0.1:2712 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\"> 0.681229</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.923497</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.0113029 </td><td style=\"text-align: right;\">           1</td><td>adamw      </td><td style=\"text-align: right;\">        10</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.292387  </td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        670.46  </td><td style=\"text-align: right;\">  1.49677 </td><td style=\"text-align: right;\">  39.8478 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00028</td><td>TERMINATED</td><td>127.0.0.1:10368</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.367797</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.949017</td><td style=\"text-align: right;\">           50</td><td style=\"text-align: right;\">0.428825  </td><td style=\"text-align: right;\">           3</td><td>adamw      </td><td style=\"text-align: right;\">        10</td><td>exp              </td><td style=\"text-align: right;\">    0.0204345 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         55.782 </td><td style=\"text-align: right;\">  2.31783 </td><td style=\"text-align: right;\">  14.4587 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00029</td><td>TERMINATED</td><td>127.0.0.1:19488</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.356034</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.928503</td><td style=\"text-align: right;\">          300</td><td style=\"text-align: right;\">0.244004  </td><td style=\"text-align: right;\">           3</td><td>adam       </td><td style=\"text-align: right;\">        10</td><td>exp              </td><td style=\"text-align: right;\">    0.084915  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         94.0647</td><td style=\"text-align: right;\">  2.09609 </td><td style=\"text-align: right;\">   2.30024</td></tr>\n",
       "<tr><td>train_model_cf6ae_00030</td><td>TERMINATED</td><td>127.0.0.1:6392 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.623744</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.934851</td><td style=\"text-align: right;\">          250</td><td style=\"text-align: right;\">0.0220837 </td><td style=\"text-align: right;\">           2</td><td>adamw      </td><td style=\"text-align: right;\">         5</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.026179  </td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        700.36  </td><td style=\"text-align: right;\">  1.64944 </td><td style=\"text-align: right;\">  30.3528 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00031</td><td>TERMINATED</td><td>127.0.0.1:7928 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\"> 0.656002</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.925567</td><td style=\"text-align: right;\">          300</td><td style=\"text-align: right;\">0.0835598 </td><td style=\"text-align: right;\">           2</td><td>adam       </td><td style=\"text-align: right;\">         5</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.0122326 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        103.325 </td><td style=\"text-align: right;\">  2.7599  </td><td style=\"text-align: right;\">  14.5451 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00032</td><td>TERMINATED</td><td>127.0.0.1:26020</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.635573</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.928914</td><td style=\"text-align: right;\">          300</td><td style=\"text-align: right;\">0.0718952 </td><td style=\"text-align: right;\">           1</td><td>adamw      </td><td style=\"text-align: right;\">        10</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.0426743 </td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        490.213 </td><td style=\"text-align: right;\">  1.34631 </td><td style=\"text-align: right;\">  43.7565 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00033</td><td>TERMINATED</td><td>127.0.0.1:23584</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\"> 0.482906</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.915163</td><td style=\"text-align: right;\">          250</td><td style=\"text-align: right;\">0.00593836</td><td style=\"text-align: right;\">           1</td><td>adam       </td><td style=\"text-align: right;\">         5</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.0553932 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        309.542 </td><td style=\"text-align: right;\">  1.95764 </td><td style=\"text-align: right;\">  20.3563 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00034</td><td>TERMINATED</td><td>127.0.0.1:3912 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\"> 0.316427</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.934976</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">0.0577669 </td><td style=\"text-align: right;\">           2</td><td>adamw      </td><td style=\"text-align: right;\">        10</td><td>exp              </td><td style=\"text-align: right;\">    0.434554  </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        125.889 </td><td style=\"text-align: right;\">  2.00857 </td><td style=\"text-align: right;\">  23.331  </td></tr>\n",
       "<tr><td>train_model_cf6ae_00035</td><td>TERMINATED</td><td>127.0.0.1:2688 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\"> 0.323277</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.912543</td><td style=\"text-align: right;\">           50</td><td style=\"text-align: right;\">0.0377496 </td><td style=\"text-align: right;\">           1</td><td>adam       </td><td style=\"text-align: right;\">        10</td><td>exp              </td><td style=\"text-align: right;\">    0.00725631</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        435.239 </td><td style=\"text-align: right;\">  1.64241 </td><td style=\"text-align: right;\">  29.0384 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00036</td><td>TERMINATED</td><td>127.0.0.1:27420</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\"> 0.562845</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.904694</td><td style=\"text-align: right;\">          250</td><td style=\"text-align: right;\">0.0158291 </td><td style=\"text-align: right;\">           3</td><td>adam       </td><td style=\"text-align: right;\">         5</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.0181484 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        143.327 </td><td style=\"text-align: right;\">  2.07998 </td><td style=\"text-align: right;\">   8.09409</td></tr>\n",
       "<tr><td>train_model_cf6ae_00037</td><td>TERMINATED</td><td>127.0.0.1:12608</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\"> 0.33672 </td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.928302</td><td style=\"text-align: right;\">          250</td><td style=\"text-align: right;\">0.0345279 </td><td style=\"text-align: right;\">           2</td><td>adam       </td><td style=\"text-align: right;\">         5</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.00869559</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        107.145 </td><td style=\"text-align: right;\">  2.08778 </td><td style=\"text-align: right;\">  17.4161 </td></tr>\n",
       "<tr><td>train_model_cf6ae_00038</td><td>TERMINATED</td><td>127.0.0.1:29576</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.438522</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.908767</td><td style=\"text-align: right;\">          250</td><td style=\"text-align: right;\">0.256975  </td><td style=\"text-align: right;\">           3</td><td>adamw      </td><td style=\"text-align: right;\">        10</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.168151  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        104.899 </td><td style=\"text-align: right;\">  2.08619 </td><td style=\"text-align: right;\">  14.597  </td></tr>\n",
       "<tr><td>train_model_cf6ae_00039</td><td>TERMINATED</td><td>127.0.0.1:17824</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.673375</td><td style=\"text-align: right;\">    1200</td><td style=\"text-align: right;\">0.947457</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">0.0313314 </td><td style=\"text-align: right;\">           2</td><td>adam       </td><td style=\"text-align: right;\">        10</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.0166248 </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        121.921 </td><td style=\"text-align: right;\">  1.95414 </td><td style=\"text-align: right;\">  13.1097 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=15820)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00000/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=15820)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00000/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=15820)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00000/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=15820)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00000/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=15820)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00000/checkpoint_000004)\n",
      "\u001b[36m(train_model pid=15820)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00000/checkpoint_000005)\n",
      "\u001b[36m(train_model pid=15820)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00000/checkpoint_000006)\n",
      "\u001b[36m(train_model pid=15820)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00000/checkpoint_000007)\n",
      "c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1384: RuntimeWarning: All-NaN slice encountered\n",
      "  return _nanquantile_unchecked(\n",
      "\u001b[36m(train_model pid=15820)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00000/checkpoint_000008)\n",
      "\u001b[36m(train_model pid=15820)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00000/checkpoint_000009)\n",
      "\u001b[36m(train_model pid=27232)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5469926038510866 and num_layers=1\n",
      "\u001b[36m(train_model pid=27232)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=27232)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00001/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=27232)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00001/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=27232)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00001/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=27232)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00001/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=27232)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00001/checkpoint_000004)\n",
      "\u001b[36m(train_model pid=27232)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00001/checkpoint_000005)\n",
      "\u001b[36m(train_model pid=27232)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00001/checkpoint_000006)\n",
      "\u001b[36m(train_model pid=27232)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00001/checkpoint_000007)\n",
      "\u001b[36m(train_model pid=27232)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00001/checkpoint_000008)\n",
      "\u001b[36m(train_model pid=27232)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00001/checkpoint_000009)\n",
      "\u001b[36m(train_model pid=27104)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00002/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=31536)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3260206371941118 and num_layers=1\n",
      "\u001b[36m(train_model pid=31536)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=31536)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00003/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=31536)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00003/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=676)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00004/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=4172)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00005/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=1952)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00006/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=1952)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00006/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=11344)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.35636968998990504 and num_layers=1\n",
      "\u001b[36m(train_model pid=11344)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=11344)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00007/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=11344)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00007/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=26172)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00008/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=26172)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00008/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=26172)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00008/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=26172)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00008/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=19368)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5918424713352256 and num_layers=1\n",
      "\u001b[36m(train_model pid=19368)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=19368)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00009/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=19368)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00009/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=19368)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00009/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=19368)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00009/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=19368)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00009/checkpoint_000004)\n",
      "\u001b[36m(train_model pid=19368)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00009/checkpoint_000005)\n",
      "\u001b[36m(train_model pid=19368)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00009/checkpoint_000006)\n",
      "\u001b[36m(train_model pid=19368)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00009/checkpoint_000007)\n",
      "\u001b[36m(train_model pid=28648)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00010/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=28648)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00010/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=27816)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5034282764658811 and num_layers=1\n",
      "\u001b[36m(train_model pid=27816)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=27816)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00011/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=27816)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00011/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=27816)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00011/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=27816)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00011/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=22512)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00012/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=10740)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00013/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=22116)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00014/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=22116)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00014/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=18304)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.40156616557373787 and num_layers=1\n",
      "\u001b[36m(train_model pid=18304)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=18304)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00015/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=18304)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00015/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=18304)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00015/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=18304)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00015/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=7504)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00016/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=15924)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00017/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=9244)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.42831202598869433 and num_layers=1\n",
      "\u001b[36m(train_model pid=9244)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=9244)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00018/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=27420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00019/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=27420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00019/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=27420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00019/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=27420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00019/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=27420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00019/checkpoint_000004)\n",
      "\u001b[36m(train_model pid=27420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00019/checkpoint_000005)\n",
      "\u001b[36m(train_model pid=27420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00019/checkpoint_000006)\n",
      "\u001b[36m(train_model pid=27420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00019/checkpoint_000007)\n",
      "\u001b[36m(train_model pid=27420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00019/checkpoint_000008)\n",
      "\u001b[36m(train_model pid=27420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00019/checkpoint_000009)\n",
      "\u001b[36m(train_model pid=20860)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00020/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=20860)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00020/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=20860)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00020/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=20860)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00020/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=20860)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00020/checkpoint_000004)\n",
      "\u001b[36m(train_model pid=20860)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00020/checkpoint_000005)\n",
      "\u001b[36m(train_model pid=20860)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00020/checkpoint_000006)\n",
      "\u001b[36m(train_model pid=20860)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00020/checkpoint_000007)\n",
      "\u001b[36m(train_model pid=22944)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6601672228653321 and num_layers=1\n",
      "\u001b[36m(train_model pid=22944)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=22944)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00021/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=2820)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00022/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=2820)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00022/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=16164)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00023/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=16164)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00023/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=25580)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00024/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=24388)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.494696861183782 and num_layers=1\n",
      "\u001b[36m(train_model pid=24388)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=24388)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00025/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=24388)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00025/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=24388)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00025/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=24388)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00025/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=24388)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00025/checkpoint_000004)\n",
      "\u001b[36m(train_model pid=24388)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00025/checkpoint_000005)\n",
      "\u001b[36m(train_model pid=24388)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00025/checkpoint_000006)\n",
      "\u001b[36m(train_model pid=24388)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00025/checkpoint_000007)\n",
      "\u001b[36m(train_model pid=24388)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00025/checkpoint_000008)\n",
      "\u001b[36m(train_model pid=24388)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00025/checkpoint_000009)\n",
      "\u001b[36m(train_model pid=25984)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00026/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=2712)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6812287388095812 and num_layers=1\n",
      "\u001b[36m(train_model pid=2712)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=2712)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00027/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=2712)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00027/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=2712)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00027/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=2712)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00027/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=2712)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00027/checkpoint_000004)\n",
      "\u001b[36m(train_model pid=2712)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00027/checkpoint_000005)\n",
      "\u001b[36m(train_model pid=2712)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00027/checkpoint_000006)\n",
      "\u001b[36m(train_model pid=2712)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00027/checkpoint_000007)\n",
      "\u001b[36m(train_model pid=10368)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00028/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=19488)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00029/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=6392)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00030/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=6392)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00030/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=6392)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00030/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=6392)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00030/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=6392)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00030/checkpoint_000004)\n",
      "\u001b[36m(train_model pid=6392)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00030/checkpoint_000005)\n",
      "\u001b[36m(train_model pid=6392)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00030/checkpoint_000006)\n",
      "\u001b[36m(train_model pid=6392)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00030/checkpoint_000007)\n",
      "\u001b[36m(train_model pid=7928)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00031/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=26020)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6355734008277453 and num_layers=1\n",
      "\u001b[36m(train_model pid=26020)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=26020)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00032/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=26020)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00032/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=26020)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00032/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=26020)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00032/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=26020)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00032/checkpoint_000004)\n",
      "\u001b[36m(train_model pid=26020)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00032/checkpoint_000005)\n",
      "\u001b[36m(train_model pid=26020)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00032/checkpoint_000006)\n",
      "\u001b[36m(train_model pid=26020)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00032/checkpoint_000007)\n",
      "\u001b[36m(train_model pid=23584)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.48290606464549135 and num_layers=1\n",
      "\u001b[36m(train_model pid=23584)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=23584)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00033/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=23584)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00033/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=23584)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00033/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=23584)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00033/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=3912)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00034/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=3912)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00034/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=2688)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.32327743820337745 and num_layers=1\n",
      "\u001b[36m(train_model pid=2688)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=2688)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00035/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=2688)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00035/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=2688)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00035/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=2688)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00035/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=27420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00036/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=12608)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00037/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=29576)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00038/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=17824)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00039/checkpoint_000000)\n",
      "2024-07-30 00:21:58,932\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM' in 0.0497s.\n",
      "\u001b[36m(train_model pid=17824)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_cf6ae_00039/checkpoint_000001)\n",
      "2024-07-30 00:21:58,972\tINFO tune.py:1041 -- Total run time: 12259.04 seconds (12258.89 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'optimizer': 'adamw', 'lr': 0.007696965501070433, 'scheduler': 'exp', 'gamma': 0.9318166309092948, 'patience': 10, 'epochs': 1200, 'batch_size': 64, 'weight_decay': 0.07563306484196267, 'hidden_size': 300, 'num_layers': 1, 'dropout': 0.494696861183782}\n",
      "Best trial final validation loss: 0.7462075460743118\n",
      "Best trial final validation accuracy: 67.2951%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.494696861183782 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8065, Test Accuracy: 68.52%\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to tuple.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmytuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LEGION\\Documents\\Msc Data Science - Uni of Exeter\\ECMM451 - Data Science Research Project (2023)\\workspace\\src\\hyperparam.py:240\u001b[0m, in \u001b[0;36mRayTuning.main\u001b[1;34m(self, train_datasets, valid_datasets, test_dataset, num_samples, max_num_epochs)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m       \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial final validation accuracy: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(best_result\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LEGION\\Documents\\Msc Data Science - Uni of Exeter\\ECMM451 - Data Science Research Project (2023)\\workspace\\src\\hyperparam.py:196\u001b[0m, in \u001b[0;36mRayTuning.test_model\u001b[1;34m(self, best_result, test_dataset)\u001b[0m\n\u001b[0;32m    194\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine(best_trained_model, optimizer, scheduler, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion, device)\n\u001b[0;32m    195\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mtest(test_loader)\n\u001b[1;32m--> 196\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBest trial test set accuracy: \u001b[39;49m\u001b[38;5;132;43;01m{:.4f}\u001b[39;49;00m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_acc\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported format string passed to tuple.__format__"
     ]
    }
   ],
   "source": [
    "mytuner.main(train_datasets, valid_datasets, test_datasets, num_samples=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like it favours;\n",
    "* num_layers = 1\n",
    "* batch_size = 64\n",
    "* hidden_size = 200-300\n",
    "* learning rate = 0.00769697 - 0.00965392 \n",
    "* weight_decay(*) = 0.005 - 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the performance of the 'best config'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dl = DataLoader(train_datasets, batch_size, shuffle=True, num_workers=4)\n",
    "valid_dl = DataLoader(valid_datasets, batch_size, shuffle=True, num_workers=4)\n",
    "test_dl = DataLoader(test_datasets, batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.494697 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "model = BiLSTMNetwork(input_size=6, hidden_size=300, num_layers=1, dropout=0.494697)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.00769697, weight_decay=0.0756331)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.931817)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = biLSTM_engine(model, optimizer, scheduler, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch [1/1200], Train Loss: 1.4008, Train Accuracy: 41.19%, Val Loss: 1.1682, Val Accuracy: 50.50%\n",
      "   Epoch [2/1200], Train Loss: 1.1863, Train Accuracy: 51.04%, Val Loss: 1.1167, Val Accuracy: 52.87%\n",
      "   Epoch [3/1200], Train Loss: 1.0640, Train Accuracy: 55.16%, Val Loss: 0.9979, Val Accuracy: 56.42%\n",
      "   Epoch [4/1200], Train Loss: 1.0017, Train Accuracy: 57.49%, Val Loss: 0.9553, Val Accuracy: 59.08%\n",
      "   Epoch [5/1200], Train Loss: 0.9555, Train Accuracy: 59.52%, Val Loss: 0.9725, Val Accuracy: 58.09%\n",
      "   Epoch [6/1200], Train Loss: 0.8975, Train Accuracy: 62.22%, Val Loss: 0.9244, Val Accuracy: 61.40%\n",
      "   Epoch [7/1200], Train Loss: 0.8870, Train Accuracy: 62.61%, Val Loss: 0.8505, Val Accuracy: 63.13%\n",
      "   Epoch [8/1200], Train Loss: 0.8216, Train Accuracy: 65.49%, Val Loss: 0.8123, Val Accuracy: 65.98%\n",
      "   Epoch [9/1200], Train Loss: 0.7931, Train Accuracy: 66.77%, Val Loss: 0.7884, Val Accuracy: 65.57%\n",
      "   Epoch [10/1200], Train Loss: 0.7606, Train Accuracy: 67.97%, Val Loss: 0.7646, Val Accuracy: 68.80%\n",
      "   Epoch [11/1200], Train Loss: 0.7464, Train Accuracy: 68.62%, Val Loss: 0.7388, Val Accuracy: 68.16%\n",
      "   Epoch [12/1200], Train Loss: 0.7284, Train Accuracy: 69.25%, Val Loss: 0.7525, Val Accuracy: 67.66%\n",
      "   Epoch [13/1200], Train Loss: 0.7004, Train Accuracy: 70.53%, Val Loss: 0.6905, Val Accuracy: 69.04%\n",
      "   Epoch [14/1200], Train Loss: 0.6793, Train Accuracy: 71.13%, Val Loss: 0.6954, Val Accuracy: 70.48%\n",
      "   Epoch [15/1200], Train Loss: 0.6629, Train Accuracy: 71.88%, Val Loss: 0.7118, Val Accuracy: 70.75%\n",
      "   Epoch [16/1200], Train Loss: 0.6541, Train Accuracy: 72.10%, Val Loss: 0.6987, Val Accuracy: 70.98%\n",
      "   Epoch [17/1200], Train Loss: 0.6324, Train Accuracy: 73.11%, Val Loss: 0.6475, Val Accuracy: 72.45%\n",
      "   Epoch [18/1200], Train Loss: 0.6292, Train Accuracy: 73.40%, Val Loss: 0.6377, Val Accuracy: 73.09%\n",
      "   Epoch [19/1200], Train Loss: 0.5988, Train Accuracy: 74.67%, Val Loss: 0.6319, Val Accuracy: 71.98%\n",
      "   Epoch [20/1200], Train Loss: 0.5909, Train Accuracy: 75.08%, Val Loss: 0.6053, Val Accuracy: 74.82%\n",
      "   Epoch [21/1200], Train Loss: 0.5689, Train Accuracy: 75.76%, Val Loss: 0.5992, Val Accuracy: 74.06%\n",
      "   Epoch [22/1200], Train Loss: 0.5548, Train Accuracy: 76.41%, Val Loss: 0.5864, Val Accuracy: 74.82%\n",
      "   Epoch [23/1200], Train Loss: 0.5417, Train Accuracy: 76.88%, Val Loss: 0.5680, Val Accuracy: 76.05%\n",
      "   Epoch [24/1200], Train Loss: 0.5309, Train Accuracy: 77.36%, Val Loss: 0.5772, Val Accuracy: 75.54%\n",
      "   Epoch [25/1200], Train Loss: 0.5212, Train Accuracy: 77.86%, Val Loss: 0.5792, Val Accuracy: 76.29%\n",
      "   Epoch [26/1200], Train Loss: 0.4988, Train Accuracy: 78.61%, Val Loss: 0.5415, Val Accuracy: 76.81%\n",
      "   Epoch [27/1200], Train Loss: 0.4911, Train Accuracy: 78.95%, Val Loss: 0.5664, Val Accuracy: 76.51%\n",
      "   Epoch [28/1200], Train Loss: 0.4741, Train Accuracy: 79.61%, Val Loss: 0.5662, Val Accuracy: 75.94%\n",
      "   Epoch [29/1200], Train Loss: 0.4617, Train Accuracy: 80.11%, Val Loss: 0.5332, Val Accuracy: 77.27%\n",
      "   Epoch [30/1200], Train Loss: 0.4550, Train Accuracy: 80.16%, Val Loss: 0.5295, Val Accuracy: 78.35%\n",
      "   Epoch [31/1200], Train Loss: 0.4587, Train Accuracy: 80.48%, Val Loss: 0.5408, Val Accuracy: 77.81%\n",
      "   Epoch [32/1200], Train Loss: 0.4503, Train Accuracy: 80.69%, Val Loss: 0.5360, Val Accuracy: 78.57%\n",
      "   Epoch [33/1200], Train Loss: 0.4241, Train Accuracy: 81.63%, Val Loss: 0.5345, Val Accuracy: 77.78%\n",
      "   Epoch [34/1200], Train Loss: 0.4087, Train Accuracy: 82.28%, Val Loss: 0.5117, Val Accuracy: 78.68%\n",
      "   Epoch [35/1200], Train Loss: 0.3986, Train Accuracy: 82.76%, Val Loss: 0.5224, Val Accuracy: 78.47%\n",
      "   Epoch [36/1200], Train Loss: 0.5469, Train Accuracy: 76.05%, Val Loss: 0.5880, Val Accuracy: 75.01%\n",
      "   Epoch [37/1200], Train Loss: 0.4404, Train Accuracy: 80.87%, Val Loss: 0.5335, Val Accuracy: 78.33%\n",
      "   Epoch [38/1200], Train Loss: 0.3977, Train Accuracy: 82.85%, Val Loss: 0.5069, Val Accuracy: 79.76%\n",
      "   Epoch [39/1200], Train Loss: 0.3814, Train Accuracy: 83.51%, Val Loss: 0.5323, Val Accuracy: 78.31%\n",
      "   Epoch [40/1200], Train Loss: 0.3707, Train Accuracy: 84.07%, Val Loss: 0.5183, Val Accuracy: 79.42%\n",
      "   Epoch [41/1200], Train Loss: 0.3570, Train Accuracy: 84.58%, Val Loss: 0.5036, Val Accuracy: 79.26%\n",
      "   Epoch [42/1200], Train Loss: 0.3572, Train Accuracy: 84.71%, Val Loss: 0.5142, Val Accuracy: 79.89%\n",
      "   Epoch [43/1200], Train Loss: 0.3415, Train Accuracy: 85.52%, Val Loss: 0.5027, Val Accuracy: 80.08%\n",
      "   Epoch [44/1200], Train Loss: 0.3315, Train Accuracy: 85.74%, Val Loss: 0.5117, Val Accuracy: 80.40%\n",
      "   Epoch [45/1200], Train Loss: 0.3263, Train Accuracy: 86.10%, Val Loss: 0.4985, Val Accuracy: 80.80%\n",
      "   Epoch [46/1200], Train Loss: 0.3158, Train Accuracy: 86.39%, Val Loss: 0.5147, Val Accuracy: 80.37%\n",
      "   Epoch [47/1200], Train Loss: 0.3069, Train Accuracy: 86.92%, Val Loss: 0.5023, Val Accuracy: 80.68%\n",
      "   Epoch [48/1200], Train Loss: 0.3004, Train Accuracy: 87.18%, Val Loss: 0.5105, Val Accuracy: 80.80%\n",
      "   Epoch [49/1200], Train Loss: 0.2991, Train Accuracy: 87.12%, Val Loss: 0.5001, Val Accuracy: 81.56%\n",
      "   Epoch [50/1200], Train Loss: 0.2831, Train Accuracy: 87.91%, Val Loss: 0.5261, Val Accuracy: 80.65%\n",
      "   Epoch [51/1200], Train Loss: 0.2820, Train Accuracy: 87.89%, Val Loss: 0.5027, Val Accuracy: 80.79%\n",
      "   Epoch [52/1200], Train Loss: 0.2762, Train Accuracy: 88.31%, Val Loss: 0.4953, Val Accuracy: 81.43%\n",
      "   Epoch [53/1200], Train Loss: 0.2645, Train Accuracy: 88.76%, Val Loss: 0.5059, Val Accuracy: 81.44%\n",
      "   Epoch [54/1200], Train Loss: 0.2584, Train Accuracy: 89.06%, Val Loss: 0.5055, Val Accuracy: 81.51%\n",
      "   Epoch [55/1200], Train Loss: 0.2498, Train Accuracy: 89.36%, Val Loss: 0.5052, Val Accuracy: 82.01%\n",
      "   Epoch [56/1200], Train Loss: 0.2443, Train Accuracy: 89.57%, Val Loss: 0.5342, Val Accuracy: 80.98%\n",
      "   Epoch [57/1200], Train Loss: 0.2406, Train Accuracy: 89.83%, Val Loss: 0.5242, Val Accuracy: 82.17%\n",
      "   Epoch [58/1200], Train Loss: 0.2359, Train Accuracy: 90.01%, Val Loss: 0.5225, Val Accuracy: 81.17%\n",
      "   Epoch [59/1200], Train Loss: 0.2265, Train Accuracy: 90.25%, Val Loss: 0.5202, Val Accuracy: 82.62%\n",
      "   Epoch [60/1200], Train Loss: 0.2244, Train Accuracy: 90.51%, Val Loss: 0.5247, Val Accuracy: 82.26%\n",
      "   Epoch [61/1200], Train Loss: 0.2201, Train Accuracy: 90.68%, Val Loss: 0.5257, Val Accuracy: 82.27%\n",
      "   Epoch [62/1200], Train Loss: 0.2157, Train Accuracy: 90.92%, Val Loss: 0.5322, Val Accuracy: 81.77%\n",
      "   Epoch [63/1200], Train Loss: 0.2095, Train Accuracy: 91.24%, Val Loss: 0.5353, Val Accuracy: 81.93%\n",
      "   Epoch [64/1200], Train Loss: 0.2060, Train Accuracy: 91.32%, Val Loss: 0.5335, Val Accuracy: 82.27%\n",
      "   Epoch [65/1200], Train Loss: 0.2002, Train Accuracy: 91.54%, Val Loss: 0.5439, Val Accuracy: 82.12%\n",
      "   Epoch [66/1200], Train Loss: 0.1978, Train Accuracy: 91.63%, Val Loss: 0.5473, Val Accuracy: 82.27%\n",
      "   Epoch [67/1200], Train Loss: 0.1920, Train Accuracy: 91.86%, Val Loss: 0.5470, Val Accuracy: 82.15%\n",
      "   Epoch [68/1200], Train Loss: 0.1884, Train Accuracy: 92.10%, Val Loss: 0.5471, Val Accuracy: 82.05%\n",
      "   Epoch [69/1200], Train Loss: 0.1852, Train Accuracy: 92.12%, Val Loss: 0.5448, Val Accuracy: 82.50%\n",
      "   Epoch [70/1200], Train Loss: 0.1820, Train Accuracy: 92.34%, Val Loss: 0.5596, Val Accuracy: 82.32%\n",
      "   Epoch [71/1200], Train Loss: 0.1770, Train Accuracy: 92.58%, Val Loss: 0.5536, Val Accuracy: 82.17%\n",
      "   Epoch [72/1200], Train Loss: 0.1747, Train Accuracy: 92.69%, Val Loss: 0.5672, Val Accuracy: 81.84%\n",
      "   Epoch [73/1200], Train Loss: 0.1730, Train Accuracy: 92.84%, Val Loss: 0.5703, Val Accuracy: 81.96%\n",
      "   Epoch [74/1200], Train Loss: 0.1690, Train Accuracy: 92.99%, Val Loss: 0.5665, Val Accuracy: 82.13%\n",
      "   Epoch [75/1200], Train Loss: 0.1658, Train Accuracy: 93.11%, Val Loss: 0.5809, Val Accuracy: 82.20%\n",
      "   Epoch [76/1200], Train Loss: 0.1669, Train Accuracy: 93.15%, Val Loss: 0.5712, Val Accuracy: 81.98%\n",
      "   Epoch [77/1200], Train Loss: 0.1618, Train Accuracy: 93.30%, Val Loss: 0.5794, Val Accuracy: 82.24%\n",
      "   Epoch [78/1200], Train Loss: 0.1585, Train Accuracy: 93.36%, Val Loss: 0.5872, Val Accuracy: 82.03%\n",
      "   Epoch [79/1200], Train Loss: 0.1566, Train Accuracy: 93.52%, Val Loss: 0.5917, Val Accuracy: 82.39%\n",
      "   Epoch [80/1200], Train Loss: 0.1548, Train Accuracy: 93.65%, Val Loss: 0.5908, Val Accuracy: 82.03%\n",
      "   Epoch [81/1200], Train Loss: 0.1524, Train Accuracy: 93.69%, Val Loss: 0.6082, Val Accuracy: 82.01%\n",
      "   Epoch [82/1200], Train Loss: 0.1514, Train Accuracy: 93.75%, Val Loss: 0.5997, Val Accuracy: 81.91%\n",
      "   Epoch [83/1200], Train Loss: 0.1495, Train Accuracy: 93.87%, Val Loss: 0.5999, Val Accuracy: 82.08%\n",
      "   Epoch [84/1200], Train Loss: 0.1474, Train Accuracy: 93.99%, Val Loss: 0.6081, Val Accuracy: 82.03%\n",
      "   Epoch [85/1200], Train Loss: 0.1450, Train Accuracy: 94.05%, Val Loss: 0.6060, Val Accuracy: 81.82%\n",
      "   Epoch [86/1200], Train Loss: 0.1446, Train Accuracy: 94.09%, Val Loss: 0.6134, Val Accuracy: 82.20%\n",
      "   Epoch [87/1200], Train Loss: 0.1423, Train Accuracy: 94.21%, Val Loss: 0.6106, Val Accuracy: 82.00%\n",
      "   Epoch [88/1200], Train Loss: 0.1413, Train Accuracy: 94.24%, Val Loss: 0.6263, Val Accuracy: 82.01%\n",
      "   Epoch [89/1200], Train Loss: 0.1405, Train Accuracy: 94.32%, Val Loss: 0.6220, Val Accuracy: 82.24%\n",
      "   Epoch [90/1200], Train Loss: 0.1403, Train Accuracy: 94.24%, Val Loss: 0.6222, Val Accuracy: 82.08%\n",
      "   Epoch [91/1200], Train Loss: 0.1378, Train Accuracy: 94.38%, Val Loss: 0.6191, Val Accuracy: 82.12%\n",
      "   Epoch [92/1200], Train Loss: 0.1365, Train Accuracy: 94.45%, Val Loss: 0.6230, Val Accuracy: 81.96%\n",
      "   Epoch [93/1200], Train Loss: 0.1355, Train Accuracy: 94.51%, Val Loss: 0.6386, Val Accuracy: 82.05%\n",
      "   Epoch [94/1200], Train Loss: 0.1354, Train Accuracy: 94.51%, Val Loss: 0.6368, Val Accuracy: 81.87%\n",
      "   Epoch [95/1200], Train Loss: 0.1342, Train Accuracy: 94.52%, Val Loss: 0.6350, Val Accuracy: 81.70%\n",
      "   Epoch [96/1200], Train Loss: 0.1338, Train Accuracy: 94.63%, Val Loss: 0.6335, Val Accuracy: 82.07%\n",
      "   Epoch [97/1200], Train Loss: 0.1325, Train Accuracy: 94.68%, Val Loss: 0.6362, Val Accuracy: 81.93%\n",
      "   Epoch [98/1200], Train Loss: 0.1316, Train Accuracy: 94.62%, Val Loss: 0.6403, Val Accuracy: 81.87%\n",
      "   Epoch [99/1200], Train Loss: 0.1312, Train Accuracy: 94.69%, Val Loss: 0.6352, Val Accuracy: 81.91%\n",
      "   Epoch [100/1200], Train Loss: 0.1303, Train Accuracy: 94.74%, Val Loss: 0.6398, Val Accuracy: 81.87%\n",
      "   Epoch [101/1200], Train Loss: 0.1300, Train Accuracy: 94.78%, Val Loss: 0.6439, Val Accuracy: 81.87%\n",
      "   Epoch [102/1200], Train Loss: 0.1298, Train Accuracy: 94.82%, Val Loss: 0.6473, Val Accuracy: 81.98%\n",
      "   Epoch [103/1200], Train Loss: 0.1292, Train Accuracy: 94.84%, Val Loss: 0.6500, Val Accuracy: 81.82%\n",
      "   Epoch [104/1200], Train Loss: 0.1288, Train Accuracy: 94.83%, Val Loss: 0.6506, Val Accuracy: 81.77%\n",
      "   Epoch [105/1200], Train Loss: 0.1282, Train Accuracy: 94.88%, Val Loss: 0.6507, Val Accuracy: 81.94%\n",
      "   Epoch [106/1200], Train Loss: 0.1277, Train Accuracy: 94.88%, Val Loss: 0.6524, Val Accuracy: 81.72%\n",
      "   Epoch [107/1200], Train Loss: 0.1275, Train Accuracy: 94.89%, Val Loss: 0.6471, Val Accuracy: 81.74%\n",
      "   Epoch [108/1200], Train Loss: 0.1273, Train Accuracy: 94.93%, Val Loss: 0.6552, Val Accuracy: 81.75%\n",
      "   Epoch [109/1200], Train Loss: 0.1266, Train Accuracy: 94.97%, Val Loss: 0.6579, Val Accuracy: 81.68%\n",
      "   Epoch [110/1200], Train Loss: 0.1259, Train Accuracy: 94.98%, Val Loss: 0.6555, Val Accuracy: 81.77%\n",
      "   Epoch [111/1200], Train Loss: 0.1259, Train Accuracy: 95.00%, Val Loss: 0.6511, Val Accuracy: 81.75%\n",
      "   Epoch [112/1200], Train Loss: 0.1254, Train Accuracy: 94.99%, Val Loss: 0.6564, Val Accuracy: 81.72%\n",
      "   Epoch [113/1200], Train Loss: 0.1254, Train Accuracy: 95.00%, Val Loss: 0.6635, Val Accuracy: 81.67%\n",
      "   Epoch [114/1200], Train Loss: 0.1251, Train Accuracy: 95.00%, Val Loss: 0.6575, Val Accuracy: 81.58%\n",
      "   Epoch [115/1200], Train Loss: 0.1249, Train Accuracy: 95.05%, Val Loss: 0.6551, Val Accuracy: 81.68%\n",
      "   Epoch [116/1200], Train Loss: 0.1242, Train Accuracy: 95.03%, Val Loss: 0.6595, Val Accuracy: 81.65%\n",
      "   Epoch [117/1200], Train Loss: 0.1243, Train Accuracy: 95.05%, Val Loss: 0.6605, Val Accuracy: 81.67%\n",
      "   Epoch [118/1200], Train Loss: 0.1244, Train Accuracy: 95.06%, Val Loss: 0.6597, Val Accuracy: 81.62%\n",
      "   Epoch [119/1200], Train Loss: 0.1242, Train Accuracy: 95.05%, Val Loss: 0.6615, Val Accuracy: 81.62%\n",
      "   Epoch [120/1200], Train Loss: 0.1238, Train Accuracy: 95.05%, Val Loss: 0.6599, Val Accuracy: 81.65%\n",
      "   Epoch [121/1200], Train Loss: 0.1236, Train Accuracy: 95.06%, Val Loss: 0.6569, Val Accuracy: 81.63%\n",
      "   Epoch [122/1200], Train Loss: 0.1235, Train Accuracy: 95.08%, Val Loss: 0.6627, Val Accuracy: 81.58%\n",
      "   Epoch [123/1200], Train Loss: 0.1233, Train Accuracy: 95.07%, Val Loss: 0.6632, Val Accuracy: 81.70%\n",
      "   Epoch [124/1200], Train Loss: 0.1235, Train Accuracy: 95.09%, Val Loss: 0.6577, Val Accuracy: 81.67%\n",
      "   Epoch [125/1200], Train Loss: 0.1231, Train Accuracy: 95.09%, Val Loss: 0.6626, Val Accuracy: 81.58%\n",
      "   Epoch [126/1200], Train Loss: 0.1231, Train Accuracy: 95.08%, Val Loss: 0.6669, Val Accuracy: 81.62%\n",
      "   Epoch [127/1200], Train Loss: 0.1229, Train Accuracy: 95.10%, Val Loss: 0.6675, Val Accuracy: 81.55%\n",
      "   Epoch [128/1200], Train Loss: 0.1232, Train Accuracy: 95.10%, Val Loss: 0.6611, Val Accuracy: 81.56%\n",
      "   Epoch [129/1200], Train Loss: 0.1230, Train Accuracy: 95.12%, Val Loss: 0.6650, Val Accuracy: 81.55%\n",
      "   Epoch [130/1200], Train Loss: 0.1230, Train Accuracy: 95.11%, Val Loss: 0.6685, Val Accuracy: 81.53%\n",
      "   Epoch [131/1200], Train Loss: 0.1225, Train Accuracy: 95.13%, Val Loss: 0.6662, Val Accuracy: 81.53%\n",
      "   Epoch [132/1200], Train Loss: 0.1227, Train Accuracy: 95.12%, Val Loss: 0.6596, Val Accuracy: 81.56%\n",
      "   Epoch [133/1200], Train Loss: 0.1229, Train Accuracy: 95.12%, Val Loss: 0.6590, Val Accuracy: 81.55%\n",
      "   Epoch [134/1200], Train Loss: 0.1225, Train Accuracy: 95.13%, Val Loss: 0.6737, Val Accuracy: 81.56%\n",
      "   Epoch [135/1200], Train Loss: 0.1220, Train Accuracy: 95.13%, Val Loss: 0.6658, Val Accuracy: 81.60%\n",
      "   Epoch [136/1200], Train Loss: 0.1223, Train Accuracy: 95.15%, Val Loss: 0.6636, Val Accuracy: 81.55%\n",
      "   Epoch [137/1200], Train Loss: 0.1223, Train Accuracy: 95.10%, Val Loss: 0.6697, Val Accuracy: 81.56%\n",
      "   Epoch [138/1200], Train Loss: 0.1221, Train Accuracy: 95.14%, Val Loss: 0.6628, Val Accuracy: 81.60%\n",
      "   Epoch [139/1200], Train Loss: 0.1222, Train Accuracy: 95.13%, Val Loss: 0.6651, Val Accuracy: 81.58%\n",
      "   Epoch [140/1200], Train Loss: 0.1220, Train Accuracy: 95.16%, Val Loss: 0.6613, Val Accuracy: 81.53%\n",
      "   Epoch [141/1200], Train Loss: 0.1223, Train Accuracy: 95.14%, Val Loss: 0.6684, Val Accuracy: 81.48%\n",
      "   Epoch [142/1200], Train Loss: 0.1224, Train Accuracy: 95.14%, Val Loss: 0.6648, Val Accuracy: 81.55%\n",
      "   Epoch [143/1200], Train Loss: 0.1216, Train Accuracy: 95.15%, Val Loss: 0.6626, Val Accuracy: 81.58%\n",
      "   Epoch [144/1200], Train Loss: 0.1218, Train Accuracy: 95.14%, Val Loss: 0.6729, Val Accuracy: 81.56%\n",
      "   Epoch [145/1200], Train Loss: 0.1223, Train Accuracy: 95.15%, Val Loss: 0.6630, Val Accuracy: 81.56%\n",
      "   Epoch [146/1200], Train Loss: 0.1220, Train Accuracy: 95.16%, Val Loss: 0.6663, Val Accuracy: 81.53%\n",
      "   Epoch [147/1200], Train Loss: 0.1216, Train Accuracy: 95.15%, Val Loss: 0.6567, Val Accuracy: 81.53%\n",
      "   Epoch [148/1200], Train Loss: 0.1220, Train Accuracy: 95.16%, Val Loss: 0.6736, Val Accuracy: 81.53%\n",
      "   Epoch [149/1200], Train Loss: 0.1217, Train Accuracy: 95.17%, Val Loss: 0.6723, Val Accuracy: 81.56%\n",
      "   Epoch [150/1200], Train Loss: 0.1220, Train Accuracy: 95.17%, Val Loss: 0.6762, Val Accuracy: 81.60%\n",
      "   Epoch [151/1200], Train Loss: 0.1219, Train Accuracy: 95.18%, Val Loss: 0.6688, Val Accuracy: 81.55%\n",
      "   Epoch [152/1200], Train Loss: 0.1221, Train Accuracy: 95.17%, Val Loss: 0.6620, Val Accuracy: 81.58%\n",
      "   Epoch [153/1200], Train Loss: 0.1216, Train Accuracy: 95.16%, Val Loss: 0.6707, Val Accuracy: 81.55%\n",
      "   Epoch [154/1200], Train Loss: 0.1216, Train Accuracy: 95.14%, Val Loss: 0.6636, Val Accuracy: 81.55%\n",
      "   Epoch [155/1200], Train Loss: 0.1215, Train Accuracy: 95.16%, Val Loss: 0.6642, Val Accuracy: 81.53%\n",
      "   Epoch [156/1200], Train Loss: 0.1218, Train Accuracy: 95.16%, Val Loss: 0.6641, Val Accuracy: 81.55%\n",
      "   Epoch [157/1200], Train Loss: 0.1217, Train Accuracy: 95.17%, Val Loss: 0.6756, Val Accuracy: 81.56%\n",
      "   Epoch [158/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6636, Val Accuracy: 81.49%\n",
      "   Epoch [159/1200], Train Loss: 0.1218, Train Accuracy: 95.16%, Val Loss: 0.6680, Val Accuracy: 81.55%\n",
      "   Epoch [160/1200], Train Loss: 0.1214, Train Accuracy: 95.17%, Val Loss: 0.6715, Val Accuracy: 81.51%\n",
      "   Epoch [161/1200], Train Loss: 0.1219, Train Accuracy: 95.17%, Val Loss: 0.6771, Val Accuracy: 81.51%\n",
      "   Epoch [162/1200], Train Loss: 0.1218, Train Accuracy: 95.18%, Val Loss: 0.6659, Val Accuracy: 81.49%\n",
      "   Epoch [163/1200], Train Loss: 0.1216, Train Accuracy: 95.18%, Val Loss: 0.6732, Val Accuracy: 81.53%\n",
      "   Epoch [164/1200], Train Loss: 0.1216, Train Accuracy: 95.18%, Val Loss: 0.6674, Val Accuracy: 81.55%\n",
      "   Epoch [165/1200], Train Loss: 0.1214, Train Accuracy: 95.18%, Val Loss: 0.6626, Val Accuracy: 81.53%\n",
      "   Epoch [166/1200], Train Loss: 0.1214, Train Accuracy: 95.18%, Val Loss: 0.6675, Val Accuracy: 81.51%\n",
      "   Epoch [167/1200], Train Loss: 0.1220, Train Accuracy: 95.18%, Val Loss: 0.6706, Val Accuracy: 81.49%\n",
      "   Epoch [168/1200], Train Loss: 0.1219, Train Accuracy: 95.18%, Val Loss: 0.6745, Val Accuracy: 81.56%\n",
      "   Epoch [169/1200], Train Loss: 0.1219, Train Accuracy: 95.18%, Val Loss: 0.6723, Val Accuracy: 81.51%\n",
      "   Epoch [170/1200], Train Loss: 0.1217, Train Accuracy: 95.18%, Val Loss: 0.6669, Val Accuracy: 81.51%\n",
      "   Epoch [171/1200], Train Loss: 0.1217, Train Accuracy: 95.18%, Val Loss: 0.6674, Val Accuracy: 81.55%\n",
      "   Epoch [172/1200], Train Loss: 0.1215, Train Accuracy: 95.18%, Val Loss: 0.6693, Val Accuracy: 81.55%\n",
      "   Epoch [173/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6650, Val Accuracy: 81.51%\n",
      "   Epoch [174/1200], Train Loss: 0.1212, Train Accuracy: 95.18%, Val Loss: 0.6692, Val Accuracy: 81.55%\n",
      "   Epoch [175/1200], Train Loss: 0.1217, Train Accuracy: 95.17%, Val Loss: 0.6685, Val Accuracy: 81.56%\n",
      "   Epoch [176/1200], Train Loss: 0.1220, Train Accuracy: 95.17%, Val Loss: 0.6767, Val Accuracy: 81.55%\n",
      "   Epoch [177/1200], Train Loss: 0.1215, Train Accuracy: 95.18%, Val Loss: 0.6703, Val Accuracy: 81.55%\n",
      "   Epoch [178/1200], Train Loss: 0.1213, Train Accuracy: 95.17%, Val Loss: 0.6639, Val Accuracy: 81.49%\n",
      "   Epoch [179/1200], Train Loss: 0.1214, Train Accuracy: 95.18%, Val Loss: 0.6681, Val Accuracy: 81.55%\n",
      "   Epoch [180/1200], Train Loss: 0.1213, Train Accuracy: 95.19%, Val Loss: 0.6572, Val Accuracy: 81.53%\n",
      "   Epoch [181/1200], Train Loss: 0.1214, Train Accuracy: 95.18%, Val Loss: 0.6653, Val Accuracy: 81.56%\n",
      "   Epoch [182/1200], Train Loss: 0.1214, Train Accuracy: 95.19%, Val Loss: 0.6716, Val Accuracy: 81.55%\n",
      "   Epoch [183/1200], Train Loss: 0.1217, Train Accuracy: 95.18%, Val Loss: 0.6675, Val Accuracy: 81.53%\n",
      "   Epoch [184/1200], Train Loss: 0.1213, Train Accuracy: 95.18%, Val Loss: 0.6665, Val Accuracy: 81.51%\n",
      "   Epoch [185/1200], Train Loss: 0.1218, Train Accuracy: 95.18%, Val Loss: 0.6673, Val Accuracy: 81.51%\n",
      "   Epoch [186/1200], Train Loss: 0.1215, Train Accuracy: 95.18%, Val Loss: 0.6749, Val Accuracy: 81.55%\n",
      "   Epoch [187/1200], Train Loss: 0.1214, Train Accuracy: 95.19%, Val Loss: 0.6706, Val Accuracy: 81.53%\n",
      "   Epoch [188/1200], Train Loss: 0.1216, Train Accuracy: 95.18%, Val Loss: 0.6645, Val Accuracy: 81.49%\n",
      "   Epoch [189/1200], Train Loss: 0.1217, Train Accuracy: 95.18%, Val Loss: 0.6721, Val Accuracy: 81.53%\n",
      "   Epoch [190/1200], Train Loss: 0.1216, Train Accuracy: 95.18%, Val Loss: 0.6747, Val Accuracy: 81.51%\n",
      "   Epoch [191/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6702, Val Accuracy: 81.55%\n",
      "   Epoch [192/1200], Train Loss: 0.1219, Train Accuracy: 95.18%, Val Loss: 0.6647, Val Accuracy: 81.53%\n",
      "   Epoch [193/1200], Train Loss: 0.1215, Train Accuracy: 95.19%, Val Loss: 0.6741, Val Accuracy: 81.51%\n",
      "   Epoch [194/1200], Train Loss: 0.1214, Train Accuracy: 95.19%, Val Loss: 0.6626, Val Accuracy: 81.55%\n",
      "   Epoch [195/1200], Train Loss: 0.1215, Train Accuracy: 95.18%, Val Loss: 0.6703, Val Accuracy: 81.55%\n",
      "   Epoch [196/1200], Train Loss: 0.1218, Train Accuracy: 95.16%, Val Loss: 0.6686, Val Accuracy: 81.55%\n",
      "   Epoch [197/1200], Train Loss: 0.1216, Train Accuracy: 95.18%, Val Loss: 0.6649, Val Accuracy: 81.53%\n",
      "   Epoch [198/1200], Train Loss: 0.1215, Train Accuracy: 95.18%, Val Loss: 0.6717, Val Accuracy: 81.53%\n",
      "   Epoch [199/1200], Train Loss: 0.1215, Train Accuracy: 95.18%, Val Loss: 0.6658, Val Accuracy: 81.55%\n",
      "   Epoch [200/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6683, Val Accuracy: 81.51%\n",
      "   Epoch [201/1200], Train Loss: 0.1215, Train Accuracy: 95.18%, Val Loss: 0.6661, Val Accuracy: 81.51%\n",
      "   Epoch [202/1200], Train Loss: 0.1216, Train Accuracy: 95.19%, Val Loss: 0.6672, Val Accuracy: 81.53%\n",
      "   Epoch [203/1200], Train Loss: 0.1217, Train Accuracy: 95.18%, Val Loss: 0.6673, Val Accuracy: 81.51%\n",
      "   Epoch [204/1200], Train Loss: 0.1216, Train Accuracy: 95.18%, Val Loss: 0.6735, Val Accuracy: 81.51%\n",
      "   Epoch [205/1200], Train Loss: 0.1214, Train Accuracy: 95.18%, Val Loss: 0.6689, Val Accuracy: 81.51%\n",
      "   Epoch [206/1200], Train Loss: 0.1217, Train Accuracy: 95.18%, Val Loss: 0.6704, Val Accuracy: 81.51%\n",
      "   Epoch [207/1200], Train Loss: 0.1215, Train Accuracy: 95.18%, Val Loss: 0.6752, Val Accuracy: 81.53%\n",
      "   Epoch [208/1200], Train Loss: 0.1215, Train Accuracy: 95.18%, Val Loss: 0.6678, Val Accuracy: 81.53%\n",
      "   Epoch [209/1200], Train Loss: 0.1215, Train Accuracy: 95.18%, Val Loss: 0.6682, Val Accuracy: 81.51%\n",
      "   Epoch [210/1200], Train Loss: 0.1217, Train Accuracy: 95.17%, Val Loss: 0.6663, Val Accuracy: 81.49%\n",
      "   Epoch [211/1200], Train Loss: 0.1213, Train Accuracy: 95.18%, Val Loss: 0.6680, Val Accuracy: 81.53%\n",
      "   Epoch [212/1200], Train Loss: 0.1217, Train Accuracy: 95.18%, Val Loss: 0.6725, Val Accuracy: 81.55%\n",
      "   Epoch [213/1200], Train Loss: 0.1216, Train Accuracy: 95.18%, Val Loss: 0.6756, Val Accuracy: 81.49%\n",
      "   Epoch [214/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6631, Val Accuracy: 81.49%\n",
      "   Epoch [215/1200], Train Loss: 0.1219, Train Accuracy: 95.17%, Val Loss: 0.6721, Val Accuracy: 81.49%\n",
      "   Epoch [216/1200], Train Loss: 0.1213, Train Accuracy: 95.18%, Val Loss: 0.6713, Val Accuracy: 81.49%\n",
      "   Epoch [217/1200], Train Loss: 0.1215, Train Accuracy: 95.18%, Val Loss: 0.6660, Val Accuracy: 81.49%\n",
      "   Epoch [218/1200], Train Loss: 0.1217, Train Accuracy: 95.19%, Val Loss: 0.6702, Val Accuracy: 81.51%\n",
      "   Epoch [219/1200], Train Loss: 0.1216, Train Accuracy: 95.18%, Val Loss: 0.6708, Val Accuracy: 81.51%\n",
      "   Epoch [220/1200], Train Loss: 0.1212, Train Accuracy: 95.19%, Val Loss: 0.6756, Val Accuracy: 81.49%\n",
      "   Epoch [221/1200], Train Loss: 0.1213, Train Accuracy: 95.18%, Val Loss: 0.6705, Val Accuracy: 81.49%\n",
      "   Epoch [222/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6616, Val Accuracy: 81.51%\n",
      "   Epoch [223/1200], Train Loss: 0.1215, Train Accuracy: 95.18%, Val Loss: 0.6721, Val Accuracy: 81.51%\n",
      "   Epoch [224/1200], Train Loss: 0.1217, Train Accuracy: 95.18%, Val Loss: 0.6690, Val Accuracy: 81.55%\n",
      "   Epoch [225/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6632, Val Accuracy: 81.51%\n",
      "   Epoch [226/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6624, Val Accuracy: 81.55%\n",
      "   Epoch [227/1200], Train Loss: 0.1217, Train Accuracy: 95.16%, Val Loss: 0.6721, Val Accuracy: 81.53%\n",
      "   Epoch [228/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6682, Val Accuracy: 81.53%\n",
      "   Epoch [229/1200], Train Loss: 0.1212, Train Accuracy: 95.18%, Val Loss: 0.6672, Val Accuracy: 81.55%\n",
      "   Epoch [230/1200], Train Loss: 0.1217, Train Accuracy: 95.18%, Val Loss: 0.6664, Val Accuracy: 81.51%\n",
      "   Epoch [231/1200], Train Loss: 0.1216, Train Accuracy: 95.18%, Val Loss: 0.6692, Val Accuracy: 81.53%\n",
      "   Epoch [232/1200], Train Loss: 0.1216, Train Accuracy: 95.18%, Val Loss: 0.6622, Val Accuracy: 81.51%\n",
      "   Epoch [233/1200], Train Loss: 0.1216, Train Accuracy: 95.18%, Val Loss: 0.6691, Val Accuracy: 81.51%\n",
      "   Epoch [234/1200], Train Loss: 0.1218, Train Accuracy: 95.17%, Val Loss: 0.6675, Val Accuracy: 81.51%\n",
      "   Epoch [235/1200], Train Loss: 0.1217, Train Accuracy: 95.18%, Val Loss: 0.6721, Val Accuracy: 81.51%\n",
      "   Epoch [236/1200], Train Loss: 0.1215, Train Accuracy: 95.18%, Val Loss: 0.6741, Val Accuracy: 81.53%\n",
      "   Epoch [237/1200], Train Loss: 0.1213, Train Accuracy: 95.18%, Val Loss: 0.6686, Val Accuracy: 81.53%\n",
      "   Epoch [238/1200], Train Loss: 0.1217, Train Accuracy: 95.18%, Val Loss: 0.6688, Val Accuracy: 81.53%\n",
      "   Epoch [239/1200], Train Loss: 0.1217, Train Accuracy: 95.18%, Val Loss: 0.6687, Val Accuracy: 81.53%\n",
      "   Epoch [240/1200], Train Loss: 0.1220, Train Accuracy: 95.17%, Val Loss: 0.6699, Val Accuracy: 81.53%\n",
      "   Epoch [241/1200], Train Loss: 0.1217, Train Accuracy: 95.18%, Val Loss: 0.6680, Val Accuracy: 81.51%\n",
      "   Epoch [242/1200], Train Loss: 0.1214, Train Accuracy: 95.18%, Val Loss: 0.6686, Val Accuracy: 81.51%\n",
      "   Epoch [243/1200], Train Loss: 0.1214, Train Accuracy: 95.18%, Val Loss: 0.6651, Val Accuracy: 81.55%\n",
      "   Epoch [244/1200], Train Loss: 0.1217, Train Accuracy: 95.17%, Val Loss: 0.6699, Val Accuracy: 81.53%\n",
      "   Epoch [245/1200], Train Loss: 0.1220, Train Accuracy: 95.17%, Val Loss: 0.6689, Val Accuracy: 81.53%\n",
      "   Epoch [246/1200], Train Loss: 0.1217, Train Accuracy: 95.17%, Val Loss: 0.6676, Val Accuracy: 81.53%\n",
      "   Epoch [247/1200], Train Loss: 0.1216, Train Accuracy: 95.18%, Val Loss: 0.6701, Val Accuracy: 81.55%\n",
      "   Epoch [248/1200], Train Loss: 0.1215, Train Accuracy: 95.18%, Val Loss: 0.6672, Val Accuracy: 81.55%\n",
      "   Epoch [249/1200], Train Loss: 0.1215, Train Accuracy: 95.18%, Val Loss: 0.6662, Val Accuracy: 81.53%\n",
      "   Epoch [250/1200], Train Loss: 0.1214, Train Accuracy: 95.18%, Val Loss: 0.6657, Val Accuracy: 81.51%\n",
      "   Epoch [251/1200], Train Loss: 0.1216, Train Accuracy: 95.18%, Val Loss: 0.6768, Val Accuracy: 81.51%\n",
      "   Epoch [252/1200], Train Loss: 0.1218, Train Accuracy: 95.18%, Val Loss: 0.6683, Val Accuracy: 81.51%\n",
      "   Epoch [253/1200], Train Loss: 0.1215, Train Accuracy: 95.18%, Val Loss: 0.6633, Val Accuracy: 81.51%\n",
      "   Epoch [254/1200], Train Loss: 0.1213, Train Accuracy: 95.18%, Val Loss: 0.6686, Val Accuracy: 81.53%\n",
      "   Epoch [255/1200], Train Loss: 0.1214, Train Accuracy: 95.18%, Val Loss: 0.6686, Val Accuracy: 81.53%\n",
      "   Epoch [256/1200], Train Loss: 0.1216, Train Accuracy: 95.18%, Val Loss: 0.6688, Val Accuracy: 81.51%\n",
      "   Epoch [257/1200], Train Loss: 0.1214, Train Accuracy: 95.18%, Val Loss: 0.6619, Val Accuracy: 81.51%\n",
      "   Epoch [258/1200], Train Loss: 0.1212, Train Accuracy: 95.18%, Val Loss: 0.6654, Val Accuracy: 81.51%\n",
      "   Epoch [259/1200], Train Loss: 0.1217, Train Accuracy: 95.18%, Val Loss: 0.6617, Val Accuracy: 81.51%\n",
      "   Epoch [260/1200], Train Loss: 0.1218, Train Accuracy: 95.18%, Val Loss: 0.6704, Val Accuracy: 81.51%\n",
      "   Epoch [261/1200], Train Loss: 0.1215, Train Accuracy: 95.18%, Val Loss: 0.6647, Val Accuracy: 81.51%\n",
      "   Epoch [262/1200], Train Loss: 0.1215, Train Accuracy: 95.18%, Val Loss: 0.6690, Val Accuracy: 81.51%\n",
      "   Epoch [263/1200], Train Loss: 0.1213, Train Accuracy: 95.18%, Val Loss: 0.6625, Val Accuracy: 81.51%\n",
      "   Epoch [264/1200], Train Loss: 0.1213, Train Accuracy: 95.18%, Val Loss: 0.6672, Val Accuracy: 81.51%\n",
      "   Epoch [265/1200], Train Loss: 0.1215, Train Accuracy: 95.18%, Val Loss: 0.6655, Val Accuracy: 81.51%\n",
      "   Epoch [266/1200], Train Loss: 0.1212, Train Accuracy: 95.18%, Val Loss: 0.6651, Val Accuracy: 81.51%\n",
      "   Epoch [267/1200], Train Loss: 0.1213, Train Accuracy: 95.18%, Val Loss: 0.6637, Val Accuracy: 81.51%\n",
      "   Epoch [268/1200], Train Loss: 0.1215, Train Accuracy: 95.18%, Val Loss: 0.6668, Val Accuracy: 81.51%\n",
      "   Epoch [269/1200], Train Loss: 0.1216, Train Accuracy: 95.18%, Val Loss: 0.6657, Val Accuracy: 81.51%\n",
      "   Epoch [270/1200], Train Loss: 0.1213, Train Accuracy: 95.18%, Val Loss: 0.6712, Val Accuracy: 81.51%\n",
      "   Epoch [271/1200], Train Loss: 0.1217, Train Accuracy: 95.18%, Val Loss: 0.6597, Val Accuracy: 81.51%\n",
      "   Epoch [272/1200], Train Loss: 0.1213, Train Accuracy: 95.18%, Val Loss: 0.6703, Val Accuracy: 81.51%\n",
      "   Epoch [273/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6656, Val Accuracy: 81.51%\n",
      "   Epoch [274/1200], Train Loss: 0.1213, Train Accuracy: 95.17%, Val Loss: 0.6645, Val Accuracy: 81.51%\n",
      "   Epoch [275/1200], Train Loss: 0.1218, Train Accuracy: 95.17%, Val Loss: 0.6642, Val Accuracy: 81.51%\n",
      "   Epoch [276/1200], Train Loss: 0.1214, Train Accuracy: 95.17%, Val Loss: 0.6666, Val Accuracy: 81.51%\n",
      "   Epoch [277/1200], Train Loss: 0.1218, Train Accuracy: 95.17%, Val Loss: 0.6646, Val Accuracy: 81.51%\n",
      "   Epoch [278/1200], Train Loss: 0.1219, Train Accuracy: 95.17%, Val Loss: 0.6677, Val Accuracy: 81.51%\n",
      "   Epoch [279/1200], Train Loss: 0.1214, Train Accuracy: 95.17%, Val Loss: 0.6690, Val Accuracy: 81.51%\n",
      "   Epoch [280/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6669, Val Accuracy: 81.51%\n",
      "   Epoch [281/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6665, Val Accuracy: 81.51%\n",
      "   Epoch [282/1200], Train Loss: 0.1213, Train Accuracy: 95.17%, Val Loss: 0.6683, Val Accuracy: 81.51%\n",
      "   Epoch [283/1200], Train Loss: 0.1217, Train Accuracy: 95.17%, Val Loss: 0.6731, Val Accuracy: 81.51%\n",
      "   Epoch [284/1200], Train Loss: 0.1218, Train Accuracy: 95.17%, Val Loss: 0.6672, Val Accuracy: 81.51%\n",
      "   Epoch [285/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6665, Val Accuracy: 81.51%\n",
      "   Epoch [286/1200], Train Loss: 0.1211, Train Accuracy: 95.17%, Val Loss: 0.6690, Val Accuracy: 81.51%\n",
      "   Epoch [287/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6676, Val Accuracy: 81.51%\n",
      "   Epoch [288/1200], Train Loss: 0.1219, Train Accuracy: 95.17%, Val Loss: 0.6744, Val Accuracy: 81.51%\n",
      "   Epoch [289/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6680, Val Accuracy: 81.51%\n",
      "   Epoch [290/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6706, Val Accuracy: 81.51%\n",
      "   Epoch [291/1200], Train Loss: 0.1211, Train Accuracy: 95.17%, Val Loss: 0.6733, Val Accuracy: 81.51%\n",
      "   Epoch [292/1200], Train Loss: 0.1217, Train Accuracy: 95.17%, Val Loss: 0.6671, Val Accuracy: 81.51%\n",
      "   Epoch [293/1200], Train Loss: 0.1218, Train Accuracy: 95.17%, Val Loss: 0.6740, Val Accuracy: 81.51%\n",
      "   Epoch [294/1200], Train Loss: 0.1212, Train Accuracy: 95.17%, Val Loss: 0.6712, Val Accuracy: 81.51%\n",
      "   Epoch [295/1200], Train Loss: 0.1214, Train Accuracy: 95.17%, Val Loss: 0.6687, Val Accuracy: 81.51%\n",
      "   Epoch [296/1200], Train Loss: 0.1217, Train Accuracy: 95.17%, Val Loss: 0.6723, Val Accuracy: 81.51%\n",
      "   Epoch [297/1200], Train Loss: 0.1219, Train Accuracy: 95.17%, Val Loss: 0.6710, Val Accuracy: 81.51%\n",
      "   Epoch [298/1200], Train Loss: 0.1214, Train Accuracy: 95.17%, Val Loss: 0.6643, Val Accuracy: 81.51%\n",
      "   Epoch [299/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6660, Val Accuracy: 81.51%\n",
      "   Epoch [300/1200], Train Loss: 0.1213, Train Accuracy: 95.17%, Val Loss: 0.6681, Val Accuracy: 81.51%\n",
      "   Epoch [301/1200], Train Loss: 0.1211, Train Accuracy: 95.17%, Val Loss: 0.6686, Val Accuracy: 81.51%\n",
      "   Epoch [302/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6754, Val Accuracy: 81.51%\n",
      "   Epoch [303/1200], Train Loss: 0.1217, Train Accuracy: 95.17%, Val Loss: 0.6652, Val Accuracy: 81.51%\n",
      "   Epoch [304/1200], Train Loss: 0.1214, Train Accuracy: 95.17%, Val Loss: 0.6702, Val Accuracy: 81.51%\n",
      "   Epoch [305/1200], Train Loss: 0.1217, Train Accuracy: 95.17%, Val Loss: 0.6783, Val Accuracy: 81.51%\n",
      "   Epoch [306/1200], Train Loss: 0.1217, Train Accuracy: 95.17%, Val Loss: 0.6661, Val Accuracy: 81.51%\n",
      "   Epoch [307/1200], Train Loss: 0.1214, Train Accuracy: 95.17%, Val Loss: 0.6783, Val Accuracy: 81.51%\n",
      "   Epoch [308/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6688, Val Accuracy: 81.51%\n",
      "   Epoch [309/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6592, Val Accuracy: 81.51%\n",
      "   Epoch [310/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6667, Val Accuracy: 81.51%\n",
      "   Epoch [311/1200], Train Loss: 0.1214, Train Accuracy: 95.17%, Val Loss: 0.6686, Val Accuracy: 81.51%\n",
      "   Epoch [312/1200], Train Loss: 0.1217, Train Accuracy: 95.17%, Val Loss: 0.6737, Val Accuracy: 81.51%\n",
      "   Epoch [313/1200], Train Loss: 0.1218, Train Accuracy: 95.17%, Val Loss: 0.6623, Val Accuracy: 81.51%\n",
      "   Epoch [314/1200], Train Loss: 0.1214, Train Accuracy: 95.17%, Val Loss: 0.6653, Val Accuracy: 81.51%\n",
      "   Epoch [315/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6700, Val Accuracy: 81.51%\n",
      "   Epoch [316/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6666, Val Accuracy: 81.51%\n",
      "   Epoch [317/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6719, Val Accuracy: 81.51%\n",
      "   Epoch [318/1200], Train Loss: 0.1212, Train Accuracy: 95.17%, Val Loss: 0.6681, Val Accuracy: 81.51%\n",
      "   Epoch [319/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6667, Val Accuracy: 81.51%\n",
      "   Epoch [320/1200], Train Loss: 0.1214, Train Accuracy: 95.17%, Val Loss: 0.6703, Val Accuracy: 81.51%\n",
      "   Epoch [321/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6636, Val Accuracy: 81.51%\n",
      "   Epoch [322/1200], Train Loss: 0.1214, Train Accuracy: 95.17%, Val Loss: 0.6665, Val Accuracy: 81.51%\n",
      "   Epoch [323/1200], Train Loss: 0.1211, Train Accuracy: 95.17%, Val Loss: 0.6737, Val Accuracy: 81.51%\n",
      "   Epoch [324/1200], Train Loss: 0.1213, Train Accuracy: 95.17%, Val Loss: 0.6673, Val Accuracy: 81.51%\n",
      "   Epoch [325/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6651, Val Accuracy: 81.51%\n",
      "   Epoch [326/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6723, Val Accuracy: 81.51%\n",
      "   Epoch [327/1200], Train Loss: 0.1213, Train Accuracy: 95.17%, Val Loss: 0.6684, Val Accuracy: 81.51%\n",
      "   Epoch [328/1200], Train Loss: 0.1210, Train Accuracy: 95.17%, Val Loss: 0.6653, Val Accuracy: 81.51%\n",
      "   Epoch [329/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6696, Val Accuracy: 81.51%\n",
      "   Epoch [330/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6617, Val Accuracy: 81.51%\n",
      "   Epoch [331/1200], Train Loss: 0.1214, Train Accuracy: 95.17%, Val Loss: 0.6724, Val Accuracy: 81.51%\n",
      "   Epoch [332/1200], Train Loss: 0.1219, Train Accuracy: 95.17%, Val Loss: 0.6689, Val Accuracy: 81.51%\n",
      "   Epoch [333/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6638, Val Accuracy: 81.51%\n",
      "   Epoch [334/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6684, Val Accuracy: 81.51%\n",
      "   Epoch [335/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6665, Val Accuracy: 81.51%\n",
      "   Epoch [336/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6687, Val Accuracy: 81.51%\n",
      "   Epoch [337/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6688, Val Accuracy: 81.51%\n",
      "   Epoch [338/1200], Train Loss: 0.1213, Train Accuracy: 95.17%, Val Loss: 0.6707, Val Accuracy: 81.51%\n",
      "   Epoch [339/1200], Train Loss: 0.1217, Train Accuracy: 95.17%, Val Loss: 0.6691, Val Accuracy: 81.51%\n",
      "   Epoch [340/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6664, Val Accuracy: 81.51%\n",
      "   Epoch [341/1200], Train Loss: 0.1214, Train Accuracy: 95.17%, Val Loss: 0.6677, Val Accuracy: 81.51%\n",
      "   Epoch [342/1200], Train Loss: 0.1218, Train Accuracy: 95.17%, Val Loss: 0.6736, Val Accuracy: 81.51%\n",
      "   Epoch [343/1200], Train Loss: 0.1219, Train Accuracy: 95.17%, Val Loss: 0.6676, Val Accuracy: 81.51%\n",
      "   Epoch [344/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6667, Val Accuracy: 81.51%\n",
      "   Epoch [345/1200], Train Loss: 0.1214, Train Accuracy: 95.17%, Val Loss: 0.6641, Val Accuracy: 81.51%\n",
      "   Epoch [346/1200], Train Loss: 0.1217, Train Accuracy: 95.17%, Val Loss: 0.6696, Val Accuracy: 81.51%\n",
      "   Epoch [347/1200], Train Loss: 0.1213, Train Accuracy: 95.17%, Val Loss: 0.6645, Val Accuracy: 81.51%\n",
      "   Epoch [348/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6660, Val Accuracy: 81.51%\n",
      "   Epoch [349/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6759, Val Accuracy: 81.51%\n",
      "   Epoch [350/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6680, Val Accuracy: 81.51%\n",
      "   Epoch [351/1200], Train Loss: 0.1211, Train Accuracy: 95.17%, Val Loss: 0.6640, Val Accuracy: 81.51%\n",
      "   Epoch [352/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6634, Val Accuracy: 81.51%\n",
      "   Epoch [353/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6696, Val Accuracy: 81.51%\n",
      "   Epoch [354/1200], Train Loss: 0.1213, Train Accuracy: 95.17%, Val Loss: 0.6766, Val Accuracy: 81.51%\n",
      "   Epoch [355/1200], Train Loss: 0.1214, Train Accuracy: 95.17%, Val Loss: 0.6725, Val Accuracy: 81.51%\n",
      "   Epoch [356/1200], Train Loss: 0.1219, Train Accuracy: 95.17%, Val Loss: 0.6726, Val Accuracy: 81.51%\n",
      "   Epoch [357/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6667, Val Accuracy: 81.51%\n",
      "   Epoch [358/1200], Train Loss: 0.1211, Train Accuracy: 95.17%, Val Loss: 0.6649, Val Accuracy: 81.51%\n",
      "   Epoch [359/1200], Train Loss: 0.1217, Train Accuracy: 95.17%, Val Loss: 0.6730, Val Accuracy: 81.51%\n",
      "   Epoch [360/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6681, Val Accuracy: 81.51%\n",
      "   Epoch [361/1200], Train Loss: 0.1217, Train Accuracy: 95.17%, Val Loss: 0.6651, Val Accuracy: 81.51%\n",
      "   Epoch [362/1200], Train Loss: 0.1214, Train Accuracy: 95.17%, Val Loss: 0.6667, Val Accuracy: 81.51%\n",
      "   Epoch [363/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6720, Val Accuracy: 81.51%\n",
      "   Epoch [364/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6680, Val Accuracy: 81.51%\n",
      "   Epoch [365/1200], Train Loss: 0.1213, Train Accuracy: 95.17%, Val Loss: 0.6654, Val Accuracy: 81.51%\n",
      "   Epoch [366/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6674, Val Accuracy: 81.51%\n",
      "   Epoch [367/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6646, Val Accuracy: 81.51%\n",
      "   Epoch [368/1200], Train Loss: 0.1216, Train Accuracy: 95.17%, Val Loss: 0.6664, Val Accuracy: 81.51%\n",
      "   Epoch [369/1200], Train Loss: 0.1217, Train Accuracy: 95.17%, Val Loss: 0.6694, Val Accuracy: 81.51%\n",
      "   Epoch [370/1200], Train Loss: 0.1215, Train Accuracy: 95.17%, Val Loss: 0.6718, Val Accuracy: 81.51%\n",
      "   Epoch [371/1200], Train Loss: 0.1214, Train Accuracy: 95.17%, Val Loss: 0.6709, Val Accuracy: 81.51%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LEGION\\Documents\\Msc Data Science - Uni of Exeter\\ECMM451 - Data Science Research Project (2023)\\workspace\\src\\engine.py:293\u001b[0m, in \u001b[0;36mbiLSTM_engine.train_validation\u001b[1;34m(self, train_loader, val_loader, epochs, save_path)\u001b[0m\n\u001b[0;32m    289\u001b[0m val_accuracies \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m    292\u001b[0m       \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m       train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m       train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m    295\u001b[0m       train_accuracies\u001b[38;5;241m.\u001b[39mappend(train_accuracy)\n",
      "File \u001b[1;32mc:\\Users\\LEGION\\Documents\\Msc Data Science - Uni of Exeter\\ECMM451 - Data Science Research Project (2023)\\workspace\\src\\engine.py:246\u001b[0m, in \u001b[0;36mbiLSTM_engine.train\u001b[1;34m(self, train_loader)\u001b[0m\n\u001b[0;32m    243\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m features, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m--> 246\u001b[0m       features, labels \u001b[38;5;241m=\u001b[39m \u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    248\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    249\u001b[0m       outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(features)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = engine.train_validation(train_dl, valid_dl, epochs=1200, save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Hyperparameter search space\n",
    "config = {\n",
    "    \"optimizer\": tune.choice([\"adam\", \"adamw\"]),\n",
    "    \"lr\": tune.loguniform(0.005, 0.01),\n",
    "    \"scheduler\": tune.choice([\"exp\", \"ReduceLROnPlateau\"]),\n",
    "    \"gamma\": tune.uniform(0.90, 0.95),  # for exponential scheduler\n",
    "    \"patience\": tune.choice([5, 10]),  # for ReduceLROnPlateau\n",
    "    \"epochs\": tune.choice([50]),\n",
    "    \"batch_size\": tune.choice([64, 128]),\n",
    "    \"weight_decay\": tune.loguniform(0.005, 0.01),\n",
    "    \"hidden_size\": tune.choice([200, 250, 300]),\n",
    "    \"num_layers\": tune.choice([1, 2]),  # Number of GRU layers\n",
    "    \"dropout\": tune.uniform(0.5, 1)  # Dropout rate\n",
    "}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "save_dir = 'BiLSTM'\n",
    "model = BiLSTMNetwork\n",
    "modelType = 'BiLSTM'\n",
    "engine = biLSTM_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytuner = RayTuning(config, save_dir, criterion, model, modelType, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-07-30 11:44:09</td></tr>\n",
       "<tr><td>Running for: </td><td>02:54:19.93        </td></tr>\n",
       "<tr><td>Memory:      </td><td>11.2/13.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=40<br>Bracket: Iter 8.000: -0.8757314388611301 | Iter 4.000: -1.0073390136594358 | Iter 2.000: -1.3090916923854663 | Iter 1.000: -1.4616141176742057<br>Logical resource usage: 2.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout</th><th style=\"text-align: right;\">  epochs</th><th style=\"text-align: right;\">   gamma</th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  num_layers</th><th>optimizer  </th><th style=\"text-align: right;\">  patience</th><th>scheduler        </th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_4c453_00000</td><td>TERMINATED</td><td>127.0.0.1:21012</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.787737</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.930793</td><td style=\"text-align: right;\">          300</td><td style=\"text-align: right;\">0.00789326</td><td style=\"text-align: right;\">           2</td><td>adamw      </td><td style=\"text-align: right;\">        10</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.00962095</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        882.188 </td><td style=\"text-align: right;\">1.36866 </td><td style=\"text-align: right;\">   36.4407</td></tr>\n",
       "<tr><td>train_model_4c453_00001</td><td>TERMINATED</td><td>127.0.0.1:32288</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.952675</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.927281</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.00546578</td><td style=\"text-align: right;\">           2</td><td>adamw      </td><td style=\"text-align: right;\">         5</td><td>exp              </td><td style=\"text-align: right;\">    0.00653429</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         64.7325</td><td style=\"text-align: right;\">1.51256 </td><td style=\"text-align: right;\">   32.2034</td></tr>\n",
       "<tr><td>train_model_4c453_00002</td><td>TERMINATED</td><td>127.0.0.1:3432 </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.659488</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.900911</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.00580187</td><td style=\"text-align: right;\">           2</td><td>adam       </td><td style=\"text-align: right;\">         5</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.00802741</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         66.4634</td><td style=\"text-align: right;\">1.54426 </td><td style=\"text-align: right;\">   31.2522</td></tr>\n",
       "<tr><td>train_model_4c453_00003</td><td>TERMINATED</td><td>127.0.0.1:20300</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.938736</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.914093</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.00633152</td><td style=\"text-align: right;\">           1</td><td>adamw      </td><td style=\"text-align: right;\">        10</td><td>exp              </td><td style=\"text-align: right;\">    0.00810417</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        581.674 </td><td style=\"text-align: right;\">0.842996</td><td style=\"text-align: right;\">   63.9398</td></tr>\n",
       "<tr><td>train_model_4c453_00004</td><td>TERMINATED</td><td>127.0.0.1:27420</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.706309</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.908872</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.00977779</td><td style=\"text-align: right;\">           1</td><td>adam       </td><td style=\"text-align: right;\">         5</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.00874685</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        115.782 </td><td style=\"text-align: right;\">1.48229 </td><td style=\"text-align: right;\">   38.1529</td></tr>\n",
       "<tr><td>train_model_4c453_00005</td><td>TERMINATED</td><td>127.0.0.1:22080</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.877271</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.946538</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.00747707</td><td style=\"text-align: right;\">           1</td><td>adam       </td><td style=\"text-align: right;\">         5</td><td>exp              </td><td style=\"text-align: right;\">    0.00673147</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        233.01  </td><td style=\"text-align: right;\">1.22218 </td><td style=\"text-align: right;\">   47.3193</td></tr>\n",
       "<tr><td>train_model_4c453_00006</td><td>TERMINATED</td><td>127.0.0.1:16052</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.505419</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.941323</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.00724133</td><td style=\"text-align: right;\">           2</td><td>adam       </td><td style=\"text-align: right;\">        10</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.00930142</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         77.2339</td><td style=\"text-align: right;\">1.61045 </td><td style=\"text-align: right;\">   26.9457</td></tr>\n",
       "<tr><td>train_model_4c453_00007</td><td>TERMINATED</td><td>127.0.0.1:29664</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.724223</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.947503</td><td style=\"text-align: right;\">          300</td><td style=\"text-align: right;\">0.00639451</td><td style=\"text-align: right;\">           2</td><td>adamw      </td><td style=\"text-align: right;\">        10</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.00744033</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         70.2346</td><td style=\"text-align: right;\">1.56209 </td><td style=\"text-align: right;\">   35.991 </td></tr>\n",
       "<tr><td>train_model_4c453_00008</td><td>TERMINATED</td><td>127.0.0.1:6968 </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.74721 </td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.937619</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.00733924</td><td style=\"text-align: right;\">           1</td><td>adam       </td><td style=\"text-align: right;\">         5</td><td>exp              </td><td style=\"text-align: right;\">    0.00864308</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        233.87  </td><td style=\"text-align: right;\">1.28314 </td><td style=\"text-align: right;\">   41.9924</td></tr>\n",
       "<tr><td>train_model_4c453_00009</td><td>TERMINATED</td><td>127.0.0.1:31116</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.918482</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.944385</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.00519743</td><td style=\"text-align: right;\">           2</td><td>adam       </td><td style=\"text-align: right;\">         5</td><td>exp              </td><td style=\"text-align: right;\">    0.00542264</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         76.6803</td><td style=\"text-align: right;\">1.5981  </td><td style=\"text-align: right;\">   31.3386</td></tr>\n",
       "<tr><td>train_model_4c453_00010</td><td>TERMINATED</td><td>127.0.0.1:31704</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.756663</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.911129</td><td style=\"text-align: right;\">          250</td><td style=\"text-align: right;\">0.00530016</td><td style=\"text-align: right;\">           1</td><td>adamw      </td><td style=\"text-align: right;\">         5</td><td>exp              </td><td style=\"text-align: right;\">    0.00927807</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        600.829 </td><td style=\"text-align: right;\">0.718324</td><td style=\"text-align: right;\">   69.2494</td></tr>\n",
       "<tr><td>train_model_4c453_00011</td><td>TERMINATED</td><td>127.0.0.1:24136</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.675109</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.944018</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.00648092</td><td style=\"text-align: right;\">           2</td><td>adamw      </td><td style=\"text-align: right;\">         5</td><td>exp              </td><td style=\"text-align: right;\">    0.00938022</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         76.7741</td><td style=\"text-align: right;\">1.51763 </td><td style=\"text-align: right;\">   35.0571</td></tr>\n",
       "<tr><td>train_model_4c453_00012</td><td>TERMINATED</td><td>127.0.0.1:9052 </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.626841</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.935254</td><td style=\"text-align: right;\">          250</td><td style=\"text-align: right;\">0.00856869</td><td style=\"text-align: right;\">           1</td><td>adam       </td><td style=\"text-align: right;\">        10</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.00628617</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        240.739 </td><td style=\"text-align: right;\">1.26977 </td><td style=\"text-align: right;\">   45.313 </td></tr>\n",
       "<tr><td>train_model_4c453_00013</td><td>TERMINATED</td><td>127.0.0.1:24996</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.618753</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.93836 </td><td style=\"text-align: right;\">          300</td><td style=\"text-align: right;\">0.00519899</td><td style=\"text-align: right;\">           1</td><td>adamw      </td><td style=\"text-align: right;\">         5</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.00837529</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        641.356 </td><td style=\"text-align: right;\">0.698669</td><td style=\"text-align: right;\">   71.3248</td></tr>\n",
       "<tr><td>train_model_4c453_00014</td><td>TERMINATED</td><td>127.0.0.1:10340</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.778003</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.926419</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.00609506</td><td style=\"text-align: right;\">           2</td><td>adamw      </td><td style=\"text-align: right;\">         5</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.00871824</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        529.1   </td><td style=\"text-align: right;\">1.0704  </td><td style=\"text-align: right;\">   53.5801</td></tr>\n",
       "<tr><td>train_model_4c453_00015</td><td>TERMINATED</td><td>127.0.0.1:18492</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.583108</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.945956</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.00720815</td><td style=\"text-align: right;\">           2</td><td>adamw      </td><td style=\"text-align: right;\">        10</td><td>exp              </td><td style=\"text-align: right;\">    0.00809673</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        619.574 </td><td style=\"text-align: right;\">0.980679</td><td style=\"text-align: right;\">   54.9983</td></tr>\n",
       "<tr><td>train_model_4c453_00016</td><td>TERMINATED</td><td>127.0.0.1:5516 </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.979351</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.934846</td><td style=\"text-align: right;\">          300</td><td style=\"text-align: right;\">0.00685173</td><td style=\"text-align: right;\">           2</td><td>adam       </td><td style=\"text-align: right;\">         5</td><td>exp              </td><td style=\"text-align: right;\">    0.00833411</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         71.2333</td><td style=\"text-align: right;\">1.83841 </td><td style=\"text-align: right;\">   25.5275</td></tr>\n",
       "<tr><td>train_model_4c453_00017</td><td>TERMINATED</td><td>127.0.0.1:28704</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.791828</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.916783</td><td style=\"text-align: right;\">          250</td><td style=\"text-align: right;\">0.00685703</td><td style=\"text-align: right;\">           2</td><td>adamw      </td><td style=\"text-align: right;\">        10</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.00782872</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         69.6717</td><td style=\"text-align: right;\">1.50295 </td><td style=\"text-align: right;\">   37.1844</td></tr>\n",
       "<tr><td>train_model_4c453_00018</td><td>TERMINATED</td><td>127.0.0.1:4468 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.635479</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.934908</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.00598719</td><td style=\"text-align: right;\">           2</td><td>adam       </td><td style=\"text-align: right;\">         5</td><td>exp              </td><td style=\"text-align: right;\">    0.00619651</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         76.4984</td><td style=\"text-align: right;\">1.47361 </td><td style=\"text-align: right;\">   32.2034</td></tr>\n",
       "<tr><td>train_model_4c453_00019</td><td>TERMINATED</td><td>127.0.0.1:17392</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.938331</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.920832</td><td style=\"text-align: right;\">          250</td><td style=\"text-align: right;\">0.00884257</td><td style=\"text-align: right;\">           2</td><td>adam       </td><td style=\"text-align: right;\">         5</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.00945518</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         80.9271</td><td style=\"text-align: right;\">1.79858 </td><td style=\"text-align: right;\">   20.7714</td></tr>\n",
       "<tr><td>train_model_4c453_00020</td><td>TERMINATED</td><td>127.0.0.1:21776</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.874789</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.93888 </td><td style=\"text-align: right;\">          300</td><td style=\"text-align: right;\">0.00509222</td><td style=\"text-align: right;\">           1</td><td>adam       </td><td style=\"text-align: right;\">        10</td><td>exp              </td><td style=\"text-align: right;\">    0.00989853</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        119.106 </td><td style=\"text-align: right;\">1.39761 </td><td style=\"text-align: right;\">   41.2833</td></tr>\n",
       "<tr><td>train_model_4c453_00021</td><td>TERMINATED</td><td>127.0.0.1:5620 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.53768 </td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.903176</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.00896341</td><td style=\"text-align: right;\">           2</td><td>adam       </td><td style=\"text-align: right;\">         5</td><td>exp              </td><td style=\"text-align: right;\">    0.0054861 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         76.5987</td><td style=\"text-align: right;\">1.57138 </td><td style=\"text-align: right;\">   32.5839</td></tr>\n",
       "<tr><td>train_model_4c453_00022</td><td>TERMINATED</td><td>127.0.0.1:4360 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.688925</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.945658</td><td style=\"text-align: right;\">          250</td><td style=\"text-align: right;\">0.00999804</td><td style=\"text-align: right;\">           2</td><td>adam       </td><td style=\"text-align: right;\">         5</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.00827093</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        120.161 </td><td style=\"text-align: right;\">1.69231 </td><td style=\"text-align: right;\">   25.3545</td></tr>\n",
       "<tr><td>train_model_4c453_00023</td><td>TERMINATED</td><td>127.0.0.1:16504</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.634336</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.913737</td><td style=\"text-align: right;\">          250</td><td style=\"text-align: right;\">0.00546828</td><td style=\"text-align: right;\">           2</td><td>adam       </td><td style=\"text-align: right;\">        10</td><td>exp              </td><td style=\"text-align: right;\">    0.00919053</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         72.5309</td><td style=\"text-align: right;\">1.73473 </td><td style=\"text-align: right;\">   29.9896</td></tr>\n",
       "<tr><td>train_model_4c453_00024</td><td>TERMINATED</td><td>127.0.0.1:10736</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.938327</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.941569</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.00717131</td><td style=\"text-align: right;\">           2</td><td>adamw      </td><td style=\"text-align: right;\">        10</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.00880494</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        192.024 </td><td style=\"text-align: right;\">1.62264 </td><td style=\"text-align: right;\">   31.8921</td></tr>\n",
       "<tr><td>train_model_4c453_00025</td><td>TERMINATED</td><td>127.0.0.1:29972</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.568186</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.937757</td><td style=\"text-align: right;\">          300</td><td style=\"text-align: right;\">0.00697394</td><td style=\"text-align: right;\">           1</td><td>adamw      </td><td style=\"text-align: right;\">         5</td><td>exp              </td><td style=\"text-align: right;\">    0.00814553</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        600.277 </td><td style=\"text-align: right;\">0.765587</td><td style=\"text-align: right;\">   66.6033</td></tr>\n",
       "<tr><td>train_model_4c453_00026</td><td>TERMINATED</td><td>127.0.0.1:25480</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.725455</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.919612</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.00762119</td><td style=\"text-align: right;\">           2</td><td>adamw      </td><td style=\"text-align: right;\">         5</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.00935726</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         77.1752</td><td style=\"text-align: right;\">1.53038 </td><td style=\"text-align: right;\">   32.999 </td></tr>\n",
       "<tr><td>train_model_4c453_00027</td><td>TERMINATED</td><td>127.0.0.1:9436 </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.884459</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.934882</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.0076955 </td><td style=\"text-align: right;\">           2</td><td>adamw      </td><td style=\"text-align: right;\">         5</td><td>exp              </td><td style=\"text-align: right;\">    0.0068746 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         66.0678</td><td style=\"text-align: right;\">1.55754 </td><td style=\"text-align: right;\">   34.6766</td></tr>\n",
       "<tr><td>train_model_4c453_00028</td><td>TERMINATED</td><td>127.0.0.1:30328</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.768632</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.917371</td><td style=\"text-align: right;\">          300</td><td style=\"text-align: right;\">0.00986255</td><td style=\"text-align: right;\">           1</td><td>adam       </td><td style=\"text-align: right;\">        10</td><td>exp              </td><td style=\"text-align: right;\">    0.00747243</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        128.877 </td><td style=\"text-align: right;\">1.45865 </td><td style=\"text-align: right;\">   35.3684</td></tr>\n",
       "<tr><td>train_model_4c453_00029</td><td>TERMINATED</td><td>127.0.0.1:23408</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.937159</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.934883</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.00555517</td><td style=\"text-align: right;\">           1</td><td>adamw      </td><td style=\"text-align: right;\">        10</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.00810285</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        470.22  </td><td style=\"text-align: right;\">0.898462</td><td style=\"text-align: right;\">   60.8613</td></tr>\n",
       "<tr><td>train_model_4c453_00030</td><td>TERMINATED</td><td>127.0.0.1:2884 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.515902</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.911192</td><td style=\"text-align: right;\">          250</td><td style=\"text-align: right;\">0.00563435</td><td style=\"text-align: right;\">           1</td><td>adamw      </td><td style=\"text-align: right;\">         5</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.00635112</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        875.871 </td><td style=\"text-align: right;\">0.743288</td><td style=\"text-align: right;\">   68.4192</td></tr>\n",
       "<tr><td>train_model_4c453_00031</td><td>TERMINATED</td><td>127.0.0.1:27836</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.546145</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.944711</td><td style=\"text-align: right;\">          300</td><td style=\"text-align: right;\">0.00688142</td><td style=\"text-align: right;\">           1</td><td>adam       </td><td style=\"text-align: right;\">         5</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.0062855 </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        127.955 </td><td style=\"text-align: right;\">1.34685 </td><td style=\"text-align: right;\">   38.9485</td></tr>\n",
       "<tr><td>train_model_4c453_00032</td><td>TERMINATED</td><td>127.0.0.1:32764</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.87413 </td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.907849</td><td style=\"text-align: right;\">          200</td><td style=\"text-align: right;\">0.00529154</td><td style=\"text-align: right;\">           2</td><td>adamw      </td><td style=\"text-align: right;\">        10</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.005009  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         82.3564</td><td style=\"text-align: right;\">1.47015 </td><td style=\"text-align: right;\">   34.5555</td></tr>\n",
       "<tr><td>train_model_4c453_00033</td><td>TERMINATED</td><td>127.0.0.1:7604 </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.531171</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.924226</td><td style=\"text-align: right;\">          250</td><td style=\"text-align: right;\">0.0072527 </td><td style=\"text-align: right;\">           1</td><td>adam       </td><td style=\"text-align: right;\">        10</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.0064569 </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        127.546 </td><td style=\"text-align: right;\">1.38358 </td><td style=\"text-align: right;\">   43.7392</td></tr>\n",
       "<tr><td>train_model_4c453_00034</td><td>TERMINATED</td><td>127.0.0.1:21996</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.677073</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.942222</td><td style=\"text-align: right;\">          250</td><td style=\"text-align: right;\">0.00819803</td><td style=\"text-align: right;\">           2</td><td>adam       </td><td style=\"text-align: right;\">         5</td><td>exp              </td><td style=\"text-align: right;\">    0.00510673</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         83.7477</td><td style=\"text-align: right;\">1.61849 </td><td style=\"text-align: right;\">   27.0495</td></tr>\n",
       "<tr><td>train_model_4c453_00035</td><td>TERMINATED</td><td>127.0.0.1:28752</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.767668</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.909114</td><td style=\"text-align: right;\">          250</td><td style=\"text-align: right;\">0.00602588</td><td style=\"text-align: right;\">           2</td><td>adamw      </td><td style=\"text-align: right;\">         5</td><td>exp              </td><td style=\"text-align: right;\">    0.00895466</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        713.353 </td><td style=\"text-align: right;\">0.893341</td><td style=\"text-align: right;\">   61.2591</td></tr>\n",
       "<tr><td>train_model_4c453_00036</td><td>TERMINATED</td><td>127.0.0.1:8444 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\"> 0.793421</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.917755</td><td style=\"text-align: right;\">          300</td><td style=\"text-align: right;\">0.007545  </td><td style=\"text-align: right;\">           2</td><td>adam       </td><td style=\"text-align: right;\">         5</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.0069717 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         86.9   </td><td style=\"text-align: right;\">1.63242 </td><td style=\"text-align: right;\">   21.6707</td></tr>\n",
       "<tr><td>train_model_4c453_00037</td><td>TERMINATED</td><td>127.0.0.1:20760</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.678048</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.914189</td><td style=\"text-align: right;\">          250</td><td style=\"text-align: right;\">0.00739416</td><td style=\"text-align: right;\">           1</td><td>adam       </td><td style=\"text-align: right;\">        10</td><td>ReduceLROnPlateau</td><td style=\"text-align: right;\">    0.00782367</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        142.213 </td><td style=\"text-align: right;\">1.38031 </td><td style=\"text-align: right;\">   39.8478</td></tr>\n",
       "<tr><td>train_model_4c453_00038</td><td>TERMINATED</td><td>127.0.0.1:26056</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.543944</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.905089</td><td style=\"text-align: right;\">          250</td><td style=\"text-align: right;\">0.00955079</td><td style=\"text-align: right;\">           1</td><td>adam       </td><td style=\"text-align: right;\">        10</td><td>exp              </td><td style=\"text-align: right;\">    0.0059294 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        258.412 </td><td style=\"text-align: right;\">1.28413 </td><td style=\"text-align: right;\">   43.0474</td></tr>\n",
       "<tr><td>train_model_4c453_00039</td><td>TERMINATED</td><td>127.0.0.1:7316 </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\"> 0.698928</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">0.902582</td><td style=\"text-align: right;\">          300</td><td style=\"text-align: right;\">0.00930943</td><td style=\"text-align: right;\">           2</td><td>adamw      </td><td style=\"text-align: right;\">         5</td><td>exp              </td><td style=\"text-align: right;\">    0.00810543</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        331.419 </td><td style=\"text-align: right;\">1.3228  </td><td style=\"text-align: right;\">   42.1826</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=21012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00000/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=21012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00000/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=21012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00000/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=21012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00000/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=21012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00000/checkpoint_000004)\n",
      "\u001b[36m(train_model pid=21012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00000/checkpoint_000005)\n",
      "\u001b[36m(train_model pid=21012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00000/checkpoint_000006)\n",
      "\u001b[36m(train_model pid=21012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00000/checkpoint_000007)\n",
      "\u001b[36m(train_model pid=21012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00000/checkpoint_000008)\n",
      "\u001b[36m(train_model pid=21012)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00000/checkpoint_000009)\n",
      "\u001b[36m(train_model pid=32288)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00001/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=3432)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00002/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=20300)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9387360067635264 and num_layers=1\n",
      "\u001b[36m(train_model pid=20300)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=20300)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00003/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=20300)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00003/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=20300)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00003/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=20300)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00003/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=20300)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00003/checkpoint_000004)\n",
      "\u001b[36m(train_model pid=20300)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00003/checkpoint_000005)\n",
      "\u001b[36m(train_model pid=20300)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00003/checkpoint_000006)\n",
      "\u001b[36m(train_model pid=20300)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00003/checkpoint_000007)\n",
      "\u001b[36m(train_model pid=20300)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00003/checkpoint_000008)\n",
      "\u001b[36m(train_model pid=20300)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00003/checkpoint_000009)\n",
      "\u001b[36m(train_model pid=27420)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7063088384557132 and num_layers=1\n",
      "\u001b[36m(train_model pid=27420)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=27420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00004/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=27420)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00004/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=22080)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8772714370423411 and num_layers=1\n",
      "\u001b[36m(train_model pid=22080)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=22080)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00005/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=22080)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00005/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=22080)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00005/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=22080)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00005/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=16052)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00006/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=29664)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00007/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=6968)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7472101523512907 and num_layers=1\n",
      "\u001b[36m(train_model pid=6968)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=6968)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00008/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=6968)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00008/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=6968)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00008/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=6968)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00008/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=31116)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00009/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=31704)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7566630843461964 and num_layers=1\n",
      "\u001b[36m(train_model pid=31704)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=31704)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00010/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=31704)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00010/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=31704)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00010/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=31704)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00010/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=31704)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00010/checkpoint_000004)\n",
      "\u001b[36m(train_model pid=31704)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00010/checkpoint_000005)\n",
      "\u001b[36m(train_model pid=31704)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00010/checkpoint_000006)\n",
      "\u001b[36m(train_model pid=31704)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00010/checkpoint_000007)\n",
      "\u001b[36m(train_model pid=31704)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00010/checkpoint_000008)\n",
      "\u001b[36m(train_model pid=31704)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00010/checkpoint_000009)\n",
      "\u001b[36m(train_model pid=24136)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00011/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=9052)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6268410869365694 and num_layers=1\n",
      "\u001b[36m(train_model pid=9052)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=9052)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00012/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=9052)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00012/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=9052)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00012/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=9052)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00012/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=24996)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.618753235646235 and num_layers=1\n",
      "\u001b[36m(train_model pid=24996)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=24996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00013/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=24996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00013/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=24996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00013/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=24996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00013/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=24996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00013/checkpoint_000004)\n",
      "\u001b[36m(train_model pid=24996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00013/checkpoint_000005)\n",
      "\u001b[36m(train_model pid=24996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00013/checkpoint_000006)\n",
      "\u001b[36m(train_model pid=24996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00013/checkpoint_000007)\n",
      "\u001b[36m(train_model pid=24996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00013/checkpoint_000008)\n",
      "\u001b[36m(train_model pid=24996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00013/checkpoint_000009)\n",
      "\u001b[36m(train_model pid=10340)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00014/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=10340)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00014/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=10340)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00014/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=10340)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00014/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=10340)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00014/checkpoint_000004)\n",
      "\u001b[36m(train_model pid=10340)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00014/checkpoint_000005)\n",
      "\u001b[36m(train_model pid=10340)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00014/checkpoint_000006)\n",
      "\u001b[36m(train_model pid=10340)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00014/checkpoint_000007)\n",
      "\u001b[36m(train_model pid=18492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00015/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=18492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00015/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=18492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00015/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=18492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00015/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=18492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00015/checkpoint_000004)\n",
      "\u001b[36m(train_model pid=18492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00015/checkpoint_000005)\n",
      "\u001b[36m(train_model pid=18492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00015/checkpoint_000006)\n",
      "\u001b[36m(train_model pid=18492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00015/checkpoint_000007)\n",
      "\u001b[36m(train_model pid=5516)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00016/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=28704)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00017/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=4468)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00018/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=17392)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00019/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=21776)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.874789149988036 and num_layers=1\n",
      "\u001b[36m(train_model pid=21776)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=21776)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00020/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=21776)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00020/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=5620)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00021/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=4360)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00022/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=16504)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00023/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=10736)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00024/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=10736)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00024/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=29972)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5681857377933849 and num_layers=1\n",
      "\u001b[36m(train_model pid=29972)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=29972)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00025/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=29972)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00025/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=29972)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00025/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=29972)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00025/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=29972)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00025/checkpoint_000004)\n",
      "\u001b[36m(train_model pid=29972)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00025/checkpoint_000005)\n",
      "\u001b[36m(train_model pid=29972)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00025/checkpoint_000006)\n",
      "\u001b[36m(train_model pid=29972)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00025/checkpoint_000007)\n",
      "\u001b[36m(train_model pid=29972)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00025/checkpoint_000008)\n",
      "\u001b[36m(train_model pid=29972)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00025/checkpoint_000009)\n",
      "\u001b[36m(train_model pid=25480)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00026/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=9436)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00027/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=30328)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7686317083570351 and num_layers=1\n",
      "\u001b[36m(train_model pid=30328)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=30328)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00028/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=30328)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00028/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=23408)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9371588024272086 and num_layers=1\n",
      "\u001b[36m(train_model pid=23408)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=23408)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00029/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=23408)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00029/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=23408)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00029/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=23408)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00029/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=23408)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00029/checkpoint_000004)\n",
      "\u001b[36m(train_model pid=23408)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00029/checkpoint_000005)\n",
      "\u001b[36m(train_model pid=23408)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00029/checkpoint_000006)\n",
      "\u001b[36m(train_model pid=23408)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00029/checkpoint_000007)\n",
      "\u001b[36m(train_model pid=2884)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5159023407425826 and num_layers=1\n",
      "\u001b[36m(train_model pid=2884)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=2884)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00030/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=2884)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00030/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=2884)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00030/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=2884)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00030/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=2884)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00030/checkpoint_000004)\n",
      "\u001b[36m(train_model pid=2884)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00030/checkpoint_000005)\n",
      "\u001b[36m(train_model pid=2884)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00030/checkpoint_000006)\n",
      "\u001b[36m(train_model pid=2884)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00030/checkpoint_000007)\n",
      "\u001b[36m(train_model pid=2884)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00030/checkpoint_000008)\n",
      "\u001b[36m(train_model pid=2884)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00030/checkpoint_000009)\n",
      "\u001b[36m(train_model pid=27836)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5461452931143731 and num_layers=1\n",
      "\u001b[36m(train_model pid=27836)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=27836)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00031/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=27836)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00031/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=32764)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00032/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=7604)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5311706812556207 and num_layers=1\n",
      "\u001b[36m(train_model pid=7604)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=7604)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00033/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=7604)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00033/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=21996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00034/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=28752)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00035/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=28752)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00035/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=28752)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00035/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=28752)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00035/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=28752)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00035/checkpoint_000004)\n",
      "\u001b[36m(train_model pid=28752)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00035/checkpoint_000005)\n",
      "\u001b[36m(train_model pid=28752)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00035/checkpoint_000006)\n",
      "\u001b[36m(train_model pid=28752)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00035/checkpoint_000007)\n",
      "\u001b[36m(train_model pid=8444)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00036/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=20760)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6780483629489231 and num_layers=1\n",
      "\u001b[36m(train_model pid=20760)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=20760)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00037/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=20760)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00037/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=26056)\u001b[0m c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5439438122700719 and num_layers=1\n",
      "\u001b[36m(train_model pid=26056)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\u001b[36m(train_model pid=26056)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00038/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=26056)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00038/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=26056)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00038/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=26056)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00038/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=7316)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00039/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=7316)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00039/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=7316)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00039/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=7316)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM/trial_4c453_00039/checkpoint_000003)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 11:44:09,549\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'c:/Users/LEGION/Documents/Msc Data Science - Uni of Exeter/ECMM451 - Data Science Research Project (2023)/workspace/ray_results/BiLSTM' in 0.0400s.\n",
      "2024-07-30 11:44:09,599\tINFO tune.py:1041 -- Total run time: 10460.00 seconds (10459.87 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'optimizer': 'adamw', 'lr': 0.005198989965043782, 'scheduler': 'ReduceLROnPlateau', 'gamma': 0.9383597426554285, 'patience': 5, 'epochs': 50, 'batch_size': 64, 'weight_decay': 0.008375293519355285, 'hidden_size': 300, 'num_layers': 1, 'dropout': 0.618753235646235}\n",
      "Best trial final validation loss: 0.6986686010937114\n",
      "Best trial final validation accuracy: 71.3248%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.618753235646235 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.7565, Test Accuracy: 71.53%\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to tuple.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmytuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LEGION\\Documents\\Msc Data Science - Uni of Exeter\\ECMM451 - Data Science Research Project (2023)\\workspace\\src\\hyperparam.py:240\u001b[0m, in \u001b[0;36mRayTuning.main\u001b[1;34m(self, train_datasets, valid_datasets, test_dataset, num_samples, max_num_epochs)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m       \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial final validation accuracy: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(best_result\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LEGION\\Documents\\Msc Data Science - Uni of Exeter\\ECMM451 - Data Science Research Project (2023)\\workspace\\src\\hyperparam.py:196\u001b[0m, in \u001b[0;36mRayTuning.test_model\u001b[1;34m(self, best_result, test_dataset)\u001b[0m\n\u001b[0;32m    194\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine(best_trained_model, optimizer, scheduler, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion, device)\n\u001b[0;32m    195\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mtest(test_loader)\n\u001b[1;32m--> 196\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBest trial test set accuracy: \u001b[39;49m\u001b[38;5;132;43;01m{:.4f}\u001b[39;49;00m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_acc\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported format string passed to tuple.__format__"
     ]
    }
   ],
   "source": [
    "mytuner.main(train_datasets, valid_datasets, test_datasets, num_samples=40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
